{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b49c7b",
   "metadata": {},
   "source": [
    "# COMP 3610 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198365f",
   "metadata": {},
   "source": [
    "### A PREDICTIVE MODEL FOR ELECTORAL OUTCOMES IN TRINIDAD & TOBAGO USING MACROECONOMIC INDICATORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93922ead",
   "metadata": {},
   "source": [
    "- Christophe Gittens  \n",
    "- Maia Neptune       \n",
    "- Zidane Timothy     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Program\n",
    "# %pip install requests beautifulsoup4 python-csv\n",
    "# %pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "csvs_path = \"csvs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da9e37",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5501724",
   "metadata": {},
   "source": [
    "## Electoral Data\n",
    "\n",
    "Electoral data was sourced from the [Elections and Boundaries Commission](https://ebctt.com) and collated into a single excel file and subsequently processed in the election_results.ipynb Jupyter notebook.\n",
    "\n",
    "* Election data encompases the 41 constituinces that were redistricted in 2007\n",
    "\n",
    "\n",
    "### Elections used in our predictive models -->\n",
    "\n",
    " - 2007\n",
    " - 2010\n",
    " - 2015\n",
    " - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "election_results = pd.read_excel('csvs/collated-tt-election-results.xlsx', sheet_name=['2007', '2010', '2015', '2020', '2025'], skiprows=[0,1,2,3,4])\n",
    "\n",
    "# headers = \n",
    "# Access individual sheets using sheet names\n",
    "sheet_1 = election_results['2007']\n",
    "sheet_2 = election_results['2010']\n",
    "sheet_3 = election_results['2015']\n",
    "sheet_4 = election_results['2020']\n",
    "sheet_5 = election_results['2025']  # blank sheet for 2025 with electorate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_1.to_csv('election2007.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9b0e2",
   "metadata": {},
   "source": [
    "2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb189f3",
   "metadata": {},
   "source": [
    "Sourcing macroeconomic data from the World Bank API. \n",
    "We will examine the the impact of the following:\n",
    "Trinidad and Tobago's Debt to GDP ratio.\n",
    "Trinidad and Tobago's GDP.\n",
    "Trinidad and Tobago's Inflation.\n",
    "Trinidad and Tobago's Government Expenditure\n",
    "Trinidad and Tobago's Unemployment Rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25458141",
   "metadata": {},
   "source": [
    "### Debt to GDP\n",
    "Source: [worldbank](api.worldbank.org) and the [CBTT](https://www.central-bank.org.tt/statistics/data-centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Central Government Domestic Debt to GDP Ratio (%)\",\n",
    "        \"Central Government External Debt to GDP Ratio (%)\",\n",
    "        \"Central Government Total Debt to GDP Ratio (%)\",\n",
    "        \"Contingent Liabilities Debt to GDP Ratio (%)\",\n",
    "        \"Gross Public Sector Debt to GDP Ratio (%)\", \n",
    "        \"Net Public Sector Debt to GDP Ratio (%)\"\n",
    "        ]\n",
    "df_debt_annual = pd.read_csv(r'csvs/Debt Annual  Central Bank of Trinidad and Tobago.csv',\n",
    "                             usecols = cols, low_memory = True)\n",
    "df_debt_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://api.worldbank.org/v2/country/TT/indicator/GC.XPN.TOTL.GD.ZS?format=json'\n",
    "# response = requests.get(url)\n",
    "\n",
    "# if response:\n",
    "#     data = response.json()\n",
    "#     tt_debt = data[1]\n",
    "#     df = pd.DataFrame(tt_debt)\n",
    "#     df.to_csv(os.path.join(csvs_path,r'tt_debt_to_gdp.csv'))\n",
    "\n",
    "# else:\n",
    "#     print(\"Response is not 200.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfc75c",
   "metadata": {},
   "source": [
    "### GDP\n",
    "Source: [Worldbank](api.worldbank.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e780d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# url = 'https://api.worldbank.org/v2/country/TT/indicator/NY.GDP.MKTP.CD?format=json'\n",
    "# response = requests.get(url)\n",
    "\n",
    "# if response:\n",
    "#     data = response.json()\n",
    "#     tt_gdp = data[1]\n",
    "#     df_gdp = pd.DataFrame(tt_gdp)\n",
    "#     df_gdp.to_csv(os.path.join(csvs_path,r'tt_gdp.csv'))\n",
    "\n",
    "# else:\n",
    "#     print(\"Response is not 200.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affe756",
   "metadata": {},
   "source": [
    "### Inflation\n",
    "Source: [worldbank](api.worldbank.org) and the [CBTT](https://www.central-bank.org.tt/statistics/data-centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Date\",\n",
    "        \"Annual Average Percent Change in the Index of Retail Prices - Inflation Rate (%)\",\n",
    "        \"Unemployment Rate (%)\",\"WTI Crude Oil Price (US$/bbl)\",\n",
    "        \"Henry Hub Natural Gas Price (US$/mmbtu)\",\n",
    "        \"Net Official Reserves (US$Mn)\"\n",
    "        ]\n",
    "df_inflation_annual = pd.read_csv(r'csvs/Selected Economic Indicators Annual  Central Bank of Trinidad and Tobago.csv',\n",
    "                             usecols = cols, low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.worldbank.org/v2/country/TT/indicator/FP.CPI.TOTL.ZG?format=json'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response:\n",
    "    data = response.json()\n",
    "    tt_inflation = data[1]\n",
    "    df_inflation = pd.DataFrame(tt_inflation)\n",
    "    df_inflation.to_csv(os.path.join(csvs_path,r'tt_inflation.csv'))\n",
    "\n",
    "else:\n",
    "    print(\"Response is not 200 for inflation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62043a5",
   "metadata": {},
   "source": [
    "### Unemployment rate\n",
    "Source:  [worldbank](api.worldbank.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194581a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Unemployment Rate (%)\"\n",
    "        ]\n",
    "tt_unemployment = pd.read_csv(r'csvs/Selected Economic Indicators Annual  Central Bank of Trinidad and Tobago.csv',\n",
    "                             usecols = cols, low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8507b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.worldbank.org/v2/country/TT/indicator/SL.UEM.TOTL.ZS?format=json'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response:\n",
    "    data = response.json()\n",
    "    tt_unemployment = data[1]\n",
    "    df = pd.DataFrame(tt_unemployment)\n",
    "    df.to_csv(os.path.join(csvs_path,r'tt_unemployment_rate.csv'))\n",
    "\n",
    "else:\n",
    "    print(\"Response is not 200.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e156d",
   "metadata": {},
   "source": [
    "### Government expenditure\n",
    "\n",
    "Source: [worldbank](api.worldbank.org) and [CBTT](https://www.central-bank.org.tt/statistics/data-centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09451ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expenditure = pd.read_csv(\"csvs/Public Finance Annual  Central Bank of Trinidad and Tobago.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://api.worldbank.org/v2/country/TT/indicator/GC.XPN.TOTL.GD.ZS?format=json'\n",
    "# response = requests.get(url)\n",
    "\n",
    "# if response:\n",
    "#     data = response.json()\n",
    "#     tt_expenditure = data[1]\n",
    "#     df_expenditure = pd.DataFrame(tt_expenditure)\n",
    "#     df_expenditure.to_csv(os.path.join(csvs_path,r'tt_gov_expenditure.csv'))\n",
    "\n",
    "# else:\n",
    "#     print(\"Response is not 200.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3cbac",
   "metadata": {},
   "source": [
    "### Homicide rate\n",
    "Source: Web-Scraping from [macrotrends](macrotrends.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd32df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# url ='https://www.macrotrends.net/global-metrics/countries/TTO/trinidad-and-tobago/murder-homicide-rate'\n",
    "\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, headers=headers)\n",
    "\n",
    "# if response:\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     div = soup.find('div', {'class': 'col-xs-6'})\n",
    "#     crime_data =[]\n",
    "#     if div:\n",
    "#         table = div.find_all('table', {'class':'historical_data_table table table-striped table-bordered'})\n",
    "#         table = table[0]\n",
    "#     if table:\n",
    "#         for row in table.find_all('tr')[2:]:\n",
    "#             cells = row.find_all('td')\n",
    "#             if len(cells) == 3:\n",
    "#                 year = cells[0].get_text(strip=True)\n",
    "#                 rate_per_100k = cells[1].get_text(strip=True)\n",
    "#                 annual_change = cells[2].get_text(strip=True)\n",
    "#                 crime_data.append([year,rate_per_100k,annual_change])\n",
    "\n",
    "#         df_homicide_rate = pd.DataFrame(crime_data, columns=['Year','Rate_per_100k_Population','Annual_change'])\n",
    "\n",
    "#         df_homicide_rate.to_csv(os.path.join(csvs_path,r'tt_homicide_rate.csv'), index=False)\n",
    "# else:\n",
    "#     print('Response is not 200.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88baf0",
   "metadata": {},
   "source": [
    "### Migration Rates\n",
    "Source: Web-Scraping from [macrotrends](macrotrends.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947945af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# url = \"https://www.macrotrends.net/global-metrics/countries/TTO/trinidad-and-tobago/net-migration#:~:text=The%20net%20migration%20rate%20for,a%200.35%25%20decline%20from%202022.\"\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, headers=headers)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     # print(soup.prettify())  # Check page structure\n",
    "# else:\n",
    "#     print(\"Failed to fetch page:\", response.status_code)\n",
    "\n",
    "# tables = soup.find_all(\"table\")\n",
    "\n",
    "# # ensure there are at least 2 tables before accessing the second one\n",
    "# if len(tables) >= 2:\n",
    "#     second_table = tables[1]  # Get the second table\n",
    "#     df_migration_rate = pd.read_html(str(second_table))[0]\n",
    "#     df_migration_rate.columns = [\"Year\", \"Net Migration Rate\", \"Growth Rate\"]\n",
    "#     print(df_migration_rate)\n",
    "# else:\n",
    "#     print(\"Second table not found!\")\n",
    "\n",
    "# df_migration_rate.to_csv(os.path.join(csvs_path, r'tt_migration_rate.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b696124",
   "metadata": {},
   "source": [
    "### Currency Exchange Rate\n",
    "Source: [CBTT](https://www.central-bank.org.tt/statistics/data-centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exchange_rate = pd.read_csv(\"csvs/Exchange Rates Annual  Central Bank of Trinidad and Tobago (1).csv\")\n",
    "df_exchange_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b82108",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba3929",
   "metadata": {},
   "source": [
    "From the central bank website n.d. is No Data or NA so that is taken in consideration moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81c64b",
   "metadata": {},
   "source": [
    "#### Preprocessing of electoral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eeae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1973c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sht_07, sht_10, sht_15, sht_20 = sheet_2007.iloc[3:], sheet_2010.iloc[3:], sheet_2015.iloc[3:], sheet_2020.iloc[3:]\n",
    "\n",
    "# print(sht_07)\n",
    "# print(sht_10)\n",
    "# print(sht_15)\n",
    "# print(sht_20)\n",
    "# sht_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442550a",
   "metadata": {},
   "source": [
    "Fixing headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4ac17",
   "metadata": {},
   "source": [
    "### 2007 Parliamentary Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74116c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\n",
    "            'ELECTORAL_DISTRICT', 'ELECTORATE', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "            'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES', 'P.N.M._VOTES',\n",
    "            'P.N.M._VOTES_%', 'U.N.C._VOTES','U.N.C._VOTES_%', 'C.O.P._VOTES', \n",
    "            'C.O.P._VOTES_%', 'D.A.C._VOTES', 'D.A.C._VOTES_%', 'D.N.A._VOTES', \n",
    "            'D.N.A._VOTES_%', 'I.N.D_VOTES', 'I.N.D._VOTES_%'\n",
    "        ]\n",
    "sht_07 = sheet_1\n",
    "sht_07.columns = cols\n",
    "sht_07.fillna(0, inplace=True)\n",
    "sht_07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd5235",
   "metadata": {},
   "source": [
    "### 2010 Parliamentary Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "            'ELECTORAL_DISTRICT', 'ELECTORATE', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "            'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES', 'P.N.M._VOTES',\n",
    "            'P.N.M._VOTES_%', 'U.N.C._VOTES','U.N.C._VOTES_%', 'C.O.P._VOTES', \n",
    "            'C.O.P._VOTES_%', 'N.N.V._VOTES', 'N.N.V._VOTES_%', 'T.O.P._VOTES', \n",
    "            'T.O.P._VOTES_%', 'T.H.C._VOTES', 'T.H.C._VOTES_%', 'I.N.D_VOTES', \n",
    "            'I.N.D._VOTES_%', 'T.T.N.C.P._VOTES', 'T.T.N.C.P._VOTES_%'\n",
    "        ]\n",
    "\n",
    "sht_10 = sheet_2\n",
    "sht_10.columns = cols\n",
    "sht_10.fillna(0, inplace=True)\n",
    "sht_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371bfaf",
   "metadata": {},
   "source": [
    "### 2015 Parliamentary Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf29cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "            'ELECTORAL_DISTRICT', 'ELECTORATE', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "            'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES', 'P.N.M._VOTES',\n",
    "            'P.N.M._VOTES_%', 'U.N.C._VOTES','U.N.C._VOTES_%', 'C.O.P._VOTES', \n",
    "            'C.O.P._VOTES_%', 'I.L.P._VOTES', 'I.L.P._VOTES_%', 'N.J.A.C._VOTES',\n",
    "            'N.J.A.C._VOTES_%','D.D.P._VOTES', 'D.D.P._VOTES_%', 'I.D.P._VOTES',\n",
    "            'I.D.P._VOTES_%', 'I.N.D_VOTES', 'I.N.D._VOTES_%', 'L.O.V.E._VOTES',\n",
    "            'L.O.V.E._VOTES_%', 'N.C.T._VOTES', 'N.C.T._VOTES_%','N.N.V._VOTES',\n",
    "            'N.N.V._VOTES_%','T.F._VOTES', 'T.F._VOTES_%', 'T.O.P._VOTES', \n",
    "            'T.O.P._VOTES_%', 'T.H.C._VOTES', 'T.H.C._VOTES_%', 'T.N.V._VOTES',\n",
    "            'T.N.V._VOTES_%', 'T.P.T._VOTES', 'T.P.T._VOTES_%', 'Y.E.P._VOTES', \n",
    "            'Y.E.P._VOTES_%', 'Y.O.U.R._VOTES', 'Y.O.U.R._VOTES_%',\n",
    "        ]\n",
    "sht_15 = sheet_3\n",
    "sht_15.columns = cols\n",
    "sht_15.fillna(0, inplace=True)\n",
    "sht_15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebc1ba",
   "metadata": {},
   "source": [
    "### 2020 Parliamentary Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "            'ELECTORAL_DISTRICT', 'ELECTORATE', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "            'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES', 'P.N.M._VOTES',\n",
    "            'P.N.M._VOTES_%', 'U.N.C._VOTES','U.N.C._VOTES_%','P.E.P._VOTES', \n",
    "            'P.E.P._VOTES_%', 'C.O.P._VOTES', 'C.O.P._VOTES_%', 'I.L.P._VOTES', \n",
    "            'I.L.P._VOTES_%', 'D.P.T._VOTES', 'D.P.T._VOTES_%', 'I.N.D_VOTES', \n",
    "            'I.N.D._VOTES_%','M.N.D._VOTES', 'M.N.D._VOTES_%', 'M.S.J._VOTES',\n",
    "            'M.S.J._VOTES_%', 'N.C.T._VOTES', 'N.C.T._VOTES_%','N.N.V._VOTES',\n",
    "            'N.N.V._VOTES_%','N.O.W._VOTES', 'N.O.W._VOTES_%', 'O.T.V._VOTES', \n",
    "            'O.T.V._VOTES_%', 'P.D.P._VOTES', 'P.D.P._VOTES_%', 'P.P._VOTES', \n",
    "            'P.P._VOTES_%', 'T.D.F._VOTES', 'T.D.F._VOTES_%', 'T.H.C._VOTES', \n",
    "            'T.H.C._VOTES_%', 'T.N.P._VOTES', 'T.N.P._VOTES_%', 'U.P.P._VOTES', \n",
    "            'U.P.P._VOTES_%', 'U.T.P._VOTES', 'U.T.P._VOTES_%'\n",
    "        ]\n",
    "sht_20 = sheet_4\n",
    "sht_20.columns = cols\n",
    "sht_20.fillna(0, inplace=True)\n",
    "sht_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "            'ELECTORAL_DISTRICT', 'ELECTORATE', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "            'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES', 'P.N.M._VOTES',\n",
    "            'P.N.M._VOTES_%', 'U.N.C._VOTES','U.N.C._VOTES_%','P.E.P._VOTES', \n",
    "            'P.E.P._VOTES_%', 'C.O.P._VOTES', 'C.O.P._VOTES_%','M.N.D._VOTES', \n",
    "            'M.N.D._VOTES_%', 'P.D.P._VOTES', 'P.D.P._VOTES_%'\n",
    "        ]\n",
    "sht_25 = sheet_5.copy()\n",
    "sht_25.columns = cols\n",
    "columns_to_fill = [col for col in sht_25.columns if col not in ['ELECTORAL_DISTRICT', 'ELECTORATE']]\n",
    "sht_25[columns_to_fill] = 0\n",
    "\n",
    "# Display the updated DataFrame\n",
    "sht_25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5b666",
   "metadata": {},
   "source": [
    "## Debt to GDP cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1616944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual[\"Date\"] = df_debt_annual[\"Date\"].astype(\"int64\")\n",
    "df_debt_annual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace String values with numeric values and convert to float\n",
    "df_debt_annual[[\"Contingent Liabilities Debt to GDP Ratio (%)\",\n",
    "                \"Gross Public Sector Debt to GDP Ratio (%)\",\n",
    "                \"Net Public Sector Debt to GDP Ratio (%)\"]] = df_debt_annual[[\"Contingent Liabilities Debt to GDP Ratio (%)\",\n",
    "                \"Gross Public Sector Debt to GDP Ratio (%)\",\n",
    "                \"Net Public Sector Debt to GDP Ratio (%)\"]].replace(\"n.d\",0)\n",
    "\n",
    "df_debt_annual[[\"Contingent Liabilities Debt to GDP Ratio (%)\",\n",
    "                \"Gross Public Sector Debt to GDP Ratio (%)\",\n",
    "                \"Net Public Sector Debt to GDP Ratio (%)\"]] = df_debt_annual[[\"Contingent Liabilities Debt to GDP Ratio (%)\",\n",
    "                \"Gross Public Sector Debt to GDP Ratio (%)\",\n",
    "                \"Net Public Sector Debt to GDP Ratio (%)\"]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f383a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52297abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4533112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debt_annual.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70b0a1",
   "metadata": {},
   "source": [
    "The data for debt to gdp looks cleaned enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56db76d",
   "metadata": {},
   "source": [
    "## GDP\n",
    "\n",
    "Soure: [CSO](https://cso.gov.tt/subjects/national-accounts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp = pd.read_csv(\"csvs/tt_gdp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ff7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b94a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp[\"date\"] = df_gdp[\"date\"].astype(\"int64\")\n",
    "df_gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22587708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a832d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db21632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a02e4",
   "metadata": {},
   "source": [
    "## Inflation\n",
    "\n",
    "Source: [Macrotrends](https://www.macrotrends.net/global-metrics/countries/TTO/trinidad-and-tobago/inflation-rate-cpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9919e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322da41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f778148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation[\"date\"] = df_inflation['date'].astype(\"int64\")\n",
    "df_inflation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation.isna().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baaf34",
   "metadata": {},
   "source": [
    "The commented code is for inflation extracted from the Central Bank of Trinidad and Tobago (CBTT) and is more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a361be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inflation_annual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inflation_annual.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b53b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inflation_annual.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2eab31",
   "metadata": {},
   "source": [
    "## Unemployment Values cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c16136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment = pd.read_csv(\"csvs/tt_unemployment_rate.csv\")\n",
    "\n",
    "# Convert all values in df_unemployment to more readable values\n",
    "df_unemployment['value'] = df_unemployment['value'].apply(lambda x: round(x, 2))\n",
    "df_unemployment.rename(columns={'value': 'Unemployment Rate (%)'}, inplace=True)\n",
    "df_unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment = df_unemployment.drop(columns={\"unit\", \"obs_status\", \"decimal\"})\n",
    "df_unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cec66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment[\"date\"] = df_unemployment['date'].astype(\"int64\")\n",
    "df_unemployment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff869a",
   "metadata": {},
   "source": [
    "Since we are only looking at 2000/2002 onward we can drop all the empty rows since they are not part of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.dropna(subset=['Unemployment Rate (%)'], inplace=True)\n",
    "df_unemployment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25322552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59994167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96528",
   "metadata": {},
   "source": [
    "The commented code is for inflation extractedd from the Central Bank of Trinidad and Tobago (CBTT) and is more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64011fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unemployment_annual.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9190b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unemployment_annual.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44118d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unemployment_annual.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ee039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_unemployment_annual.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa62ee1",
   "metadata": {},
   "source": [
    "## Government Expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34447c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expenditure.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expenditure['Date'] = df_expenditure['Date'].astype(\"int64\")\n",
    "df_expenditure.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expenditure.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b73af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expenditure.isna().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf81b76",
   "metadata": {},
   "source": [
    "Given the only important missing values are 2020 and 2000 expenditure values, we can source those two and drop the other rows that we are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d84c6",
   "metadata": {},
   "source": [
    "## Homicide Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db37224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_homicide_rate = pd.read_csv(r\"csvs/tt_homicide_rate.csv\") #rate per 100k for 2023 and 2023 sourced from statista.com others from macrotrend.com\n",
    "\n",
    "#pct_change will make Annual_change row for first value null\n",
    "df_homicide_rate['Annual_change'] = df_homicide_rate[\"Annual_change\"].replace(to_replace=' ', value=0)\n",
    "df_homicide_rate['Annual_change'] = df_homicide_rate[\"Annual_change\"].str.rstrip('%').astype('float')\n",
    "last_homicide_row = df_homicide_rate.iloc[-1:]\n",
    "\n",
    "df_homicide_rate= df_homicide_rate.sort_values(by='Year')\n",
    "df_homicide_rate['Annual_change'] = (df_homicide_rate['Rate_per_100k_Population'].pct_change()* 100).round(2) \n",
    "df_homicide_rate= df_homicide_rate.sort_values(by='Year', ascending=False)\n",
    "\n",
    "df_homicide_rate.iloc[-1:] = last_homicide_row\n",
    "\n",
    "df_homicide_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_homicide_by_div = pd.read_csv(r'csvs/tt_murders_by_division_2013_2024.csv')\n",
    "df_homicide_by_div['Year'] = df_homicide_by_div['Year'].astype('int64')\n",
    "df_homicide_by_div = pd.merge(df_homicide_rate, df_homicide_by_div, on='Year', how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1d01c",
   "metadata": {},
   "source": [
    "## Constituency/Division Specific Homicide Information\n",
    "Incase accuracy is low. This will be employed for the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #melting the df\n",
    "df_long_homicide_by_div = df_homicide_by_div.melt(id_vars=['Year', 'Rate_per_100k_Population', 'Annual_change','Total_murders_per_year'], \n",
    "                  var_name='Division', \n",
    "                  value_name='Murders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def murder_estimate(division, df_long):\n",
    "    division_data = df_long[df_long['Division'] == division].copy()\n",
    "\n",
    "    murders_2013 = division_data[division_data['Year'] == 2013]['Murders'].values[0]\n",
    "\n",
    "    for year in range(2012, 1999, -1):\n",
    "        try:\n",
    "            pct_change = division_data[division_data['Year'] == year + 1]['Annual_change'].values[0]\n",
    "        except IndexError:\n",
    "            continue  \n",
    "\n",
    "        projected_murders = (murders_2013 / (1 + pct_change / 100)).round()\n",
    "\n",
    "        mask = (df_long['Year'] == year) & (df_long['Division'] == division)\n",
    "        if df_long.loc[mask, 'Murders'].isna().any():\n",
    "            df_long.loc[mask, 'Murders'] = projected_murders\n",
    "        else:\n",
    "            new_row = {\n",
    "                'Year': year,\n",
    "                'Division': division,\n",
    "                'Rate_per_100k_Population': np.nan,\n",
    "                'Annual_change': np.nan,\n",
    "                'Murders': projected_murders,\n",
    "                'Annual_change': np.nan\n",
    "            }\n",
    "            df_long = pd.concat([df_long, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        murders_2013 = projected_murders\n",
    "\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisions = df_long_homicide_by_div['Division'].dropna().unique()\n",
    "\n",
    "for division in divisions:\n",
    "    df_long_homicide_by_div = murder_estimate(division, df_long_homicide_by_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see if data is accurate\n",
    "#quote \"When murder data are restricted to 2001-2013, the\n",
    "# largest proportion of murders in Trinidad and Tobago took place in the Port of Spain Division\n",
    "# (26.7 per cent). This was followed by the Northern Division (19.9 per cent), North Eastern\n",
    "# Division (13.3 per cent), and Western Division (12.3 per cent).\"\n",
    "\n",
    "subset = df_long_homicide_by_div[(df_long_homicide_by_div['Year'] >= 2001) & (df_long_homicide_by_div['Year'] <= 2013)]\n",
    "\n",
    "total_by_division = subset.groupby('Division')['Murders'].sum()\n",
    "grand_total = total_by_division.sum()\n",
    "\n",
    "division_percentages = (total_by_division / grand_total) * 100\n",
    "division_percentages = division_percentages.sort_values(ascending=False)\n",
    "print(division_percentages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08f890",
   "metadata": {},
   "source": [
    "From: 1. Crime-Trinidad and Tobago. 2. Crime prevention-Trinidad and Tobago. 3. ViolenceTrinidad and Tobago. 4. Violence-Prevention-Trinidad and Tobago. I. Inter-American\n",
    "Development Bank. Country Department Caribbean Group. II. Title. III. Series.\n",
    "IDB-TN-1062 \n",
    "\n",
    "\"When murder data are restricted to 2001-2013, the largest proportion of murders in Trinidad and Tobago took place in the Port of Spain Division (26.7 per cent). This was followed by the Northern Division (19.9 per cent), North Eastern Division (13.3 per cent), and Western Division (12.3 per cent).\"\n",
    "\n",
    "The difference in percentage in Port-Of-Spain can be accounted for due to the 2013 murder count starting off at 122 murders and slightly dropping. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982f936",
   "metadata": {},
   "source": [
    "## Migration Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate = pd.read_csv(\"csvs/tt_migration_rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f94a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6129b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate['Year'] = df_migration_rate['Year'].astype('int64')\n",
    "df_migration_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate[\"Growth Rate\"] = (\n",
    "    df_migration_rate[\"Growth Rate\"]\n",
    "    .str.rstrip('%')\n",
    "    .astype('float')\n",
    ").round(4)\n",
    "\n",
    "df_migration_rate = df_migration_rate.rename(columns={\n",
    "    \"Growth Rate\": \"Growth Rate(in percent)\",\n",
    "    \"Year\": \"Year\",\n",
    "    \"Net Migration Rate\": \"Net Migration Rate\"\n",
    "})\n",
    "\n",
    "df_migration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae955207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migration_rate.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ab2f8",
   "metadata": {},
   "source": [
    "# Putting Together Datasets for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all date/year columns are named consistently for merging\n",
    "df_debt_annual.rename(columns={\"Date\": \"Year\"}, inplace=True)\n",
    "df_gdp.rename(columns={\"date\": \"Year\"}, inplace=True)\n",
    "df_inflation.rename(columns={\"date\": \"Year\"}, inplace=True)\n",
    "df_unemployment.rename(columns={\"date\": \"Year\"}, inplace=True)\n",
    "df_expenditure.rename(columns={\"Date\": \"Year\"}, inplace=True)\n",
    "df_migration_rate.rename(columns={\"Year\": \"Year\"}, inplace=True)\n",
    "df_homicide_rate.rename(columns={\"Year\": \"Year\"}, inplace=True)\n",
    "\n",
    "# Merge all dataframes on the \"Year\" column\n",
    "dataset = df_debt_annual.merge(df_gdp, on=\"Year\", how=\"outer\") \\\n",
    "                          .merge(df_inflation, on=\"Year\", how=\"outer\") \\\n",
    "                          .merge(df_unemployment, on=\"Year\", how=\"outer\") \\\n",
    "                          .merge(df_expenditure, on=\"Year\", how=\"outer\") \\\n",
    "                          .merge(df_migration_rate, on=\"Year\", how=\"outer\") \\\n",
    "                          .merge(df_homicide_rate, on=\"Year\", how=\"outer\")\n",
    "\n",
    "# Display the merged dataframe\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to more meaningful names\n",
    "dataset.rename(columns={\n",
    "    'id_x': 'Debt_ID',\n",
    "    'id_y': 'GDP_ID',\n",
    "    'value_x': 'Debt_Value',\n",
    "    'value_y': 'GDP_Value',\n",
    "    'value': 'Inflation_Value'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the updated dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a127b45",
   "metadata": {},
   "source": [
    "Dropping all the rows and columns we do not need. In this instance values that are after 2020 and before 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1161ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(subset=['Net Public Sector Debt to GDP Ratio (%)'], inplace=True)\n",
    "dataset.dropna(subset=['Rate_per_100k_Population'], inplace=True)\n",
    "dataset.drop(columns={\"unit_x\", \"obs_status_x\", \"unit_y\", \"country_y\", \"country_x\"}, inplace=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffdb04",
   "metadata": {},
   "source": [
    "# Determining values for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815d476",
   "metadata": {},
   "source": [
    "Compiling Unemployment Average ratio from 2007 to 2010, 2010 to 2015 and 2015 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a073635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate averages for the specified periods\n",
    "unemployment_avg_07_10 = dataset[(dataset['Year'] >= 2007) & (dataset['Year'] <= 2010)]['Annual_change'].mean()\n",
    "unemployment_avg_10_15 = dataset[(dataset['Year'] > 2010) & (dataset['Year'] <= 2015)]['Annual_change'].mean()\n",
    "unemployment_avg_15_20 = dataset[(dataset['Year'] > 2015) & (dataset['Year'] <= 2020)]['Annual_change'].mean()\n",
    "unemployment_avg_20_25 = dataset[(dataset['Year'] > 2020) & (dataset['Year'] <= 2024)]['Annual_change'].mean()\n",
    "\n",
    "df_unemp_avg_07_10 = pd.DataFrame({'Period': ['2007-2010'], 'Average Unemployment Rate (%)': [unemployment_avg_07_10]})\n",
    "df_unemp_avg_10_15 = pd.DataFrame({'Period': ['2010-2015'], 'Average Unemployment Rate (%)': [unemployment_avg_10_15]})\n",
    "df_unemp_avg_15_20 = pd.DataFrame({'Period': ['2015-2020'], 'Average Unemployment Rate (%)': [unemployment_avg_15_20]})\n",
    "df_unemp_avg_20_25 = pd.DataFrame({'Period': ['2020-2024'], 'Average Unemployment Rate (%)': [unemployment_avg_20_25]})\n",
    "\n",
    "print(df_unemp_avg_07_10)\n",
    "print(df_unemp_avg_10_15)\n",
    "print(df_unemp_avg_15_20)\n",
    "print(df_unemp_avg_20_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44af8ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee539cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate averages for the specified periods\n",
    "inflation_avg_07_10 = dataset[(dataset['Year'] >= 2007) & (dataset['Year'] <= 2010)]['Annual_change'].mean()\n",
    "inflation_avg_10_15 = dataset[(dataset['Year'] > 2010) & (dataset['Year'] <= 2015)]['Annual_change'].mean()\n",
    "inflation_avg_15_20 = dataset[(dataset['Year'] > 2015) & (dataset['Year'] <= 2020)]['Annual_change'].mean()\n",
    "inflation_avg_20_25 = dataset[(dataset['Year'] > 2020) & (dataset['Year'] <= 2024)]['Annual_change'].mean()\n",
    "\n",
    "df_inf_avg_07_10 = pd.DataFrame({'Period': ['2007-2010'], 'Average Inflation Rate (%)': [inflation_avg_07_10]})\n",
    "df_inf_avg_10_15 = pd.DataFrame({'Period': ['2010-2015'], 'Average Inflation Rate (%)': [inflation_avg_10_15]})\n",
    "df_inf_avg_15_20 = pd.DataFrame({'Period': ['2015-2020'], 'Average Inflation Rate (%)': [inflation_avg_15_20]})\n",
    "df_inf_avg_20_25 = pd.DataFrame({'Period': ['2020-2024'], 'Average Inflation Rate (%)': [inflation_avg_20_25]})\n",
    "\n",
    "print(df_inf_avg_07_10)\n",
    "print(df_inf_avg_10_15)\n",
    "print(df_inf_avg_15_20)\n",
    "print(df_inf_avg_20_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b480e76",
   "metadata": {},
   "source": [
    "## Electoral Elasticity Calculations \n",
    "\n",
    "Our electoral elasticity variable is determined by a series of mathematical formulae outlined in this <a href = \"Data/Election_Forecasting_Using_Macroeconomic.pdf\"> Research Paper. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the change in vote share for P.N.M., U.N.C., and C.O.P. between constituencies for the different years\n",
    "sht_10_share = sht_10[['ELECTORAL_DISTRICT', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'C.O.P._VOTES_%']]\n",
    "sht_07_share = sht_07[['ELECTORAL_DISTRICT', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'C.O.P._VOTES_%']]\n",
    "sht_15_share = sht_15[['ELECTORAL_DISTRICT', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'C.O.P._VOTES_%']]\n",
    "sht_20_share = sht_20[['ELECTORAL_DISTRICT', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'C.O.P._VOTES_%']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715adfc",
   "metadata": {},
   "source": [
    "Calculate differences in vote percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d762d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference in vote share\n",
    "diff_1 = sht_07_share.set_index('ELECTORAL_DISTRICT').sub(sht_10_share.set_index('ELECTORAL_DISTRICT'), axis=0)\n",
    "diff_1.reset_index(inplace=True)\n",
    "diff_1.rename(columns={\n",
    "\t'P.N.M._VOTES_%': 'P.N.M._VOTES_%_CHANGE',\n",
    "\t'U.N.C._VOTES_%': 'U.N.C._VOTES_%_CHANGE',\n",
    "\t'C.O.P._VOTES_%': 'C.O.P._VOTES_%_CHANGE'\n",
    "}, inplace=True)\n",
    "\n",
    "diff_2 = sht_10_share.set_index('ELECTORAL_DISTRICT').sub(sht_15_share.set_index('ELECTORAL_DISTRICT'), axis=0)\n",
    "diff_2.reset_index(inplace=True)\n",
    "diff_2.rename(columns={\n",
    "\t'P.N.M._VOTES_%': 'P.N.M._VOTES_%_CHANGE',\n",
    "\t'U.N.C._VOTES_%': 'U.N.C._VOTES_%_CHANGE',\n",
    "\t'C.O.P._VOTES_%': 'C.O.P._VOTES_%_CHANGE'\n",
    "}, inplace=True)\n",
    "\n",
    "diff_3 = sht_15_share.set_index('ELECTORAL_DISTRICT').sub(sht_20_share.set_index('ELECTORAL_DISTRICT'), axis=0)\n",
    "diff_3.reset_index(inplace=True)\n",
    "diff_3.rename(columns={\n",
    "\t'P.N.M._VOTES_%': 'P.N.M._VOTES_%_CHANGE',\n",
    "\t'U.N.C._VOTES_%': 'U.N.C._VOTES_%_CHANGE',\n",
    "\t'C.O.P._VOTES_%': 'C.O.P._VOTES_%_CHANGE'\n",
    "}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf9cfb",
   "metadata": {},
   "source": [
    "Output of Elasticity Change in  Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c19a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad909",
   "metadata": {},
   "source": [
    "## Creation of master dataframe and model training follows\n",
    "\n",
    "The master DataFrame consolidates various social, electoral and economic data for Trinidad and Tobago over a range of years. It is designed to provide a comprehensive dataset for analysis and modeling. Below is a breakdown of the key components and columns in the DataFrame:\n",
    "\n",
    "### Columns:\n",
    "1. **Year**: The year corresponding to the data.\n",
    "2. **GDP (Per Capita US$)**: The Gross Domestic Product per capita in US dollars.\n",
    "3. **GNI (Per Capita US $)**: The Gross National Income per capita in US dollars.\n",
    "4. **GNI (Percentage Change)**: The year-over-year percentage change in GNI.\n",
    "5. **Homicide_rate_per_100k**: The homicide rate per 100,000 people.\n",
    "6. **Inflation**: Rate of Chnage in prices.\n",
    "7. **Unemployment**: Rate of unemployment.\n",
    "8. **Net Migrantion**: The difference between those leaving and those coming to reside in Trinidad and Tobago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd46a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomic_df = pd.read_csv(r\"csvs/socio-economic-indicators-collated copy.csv\")\n",
    "socioeconomic_df.columns = socioeconomic_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_gdp_df = pd.read_csv(r\"csvs/tt_gdp.csv\")\n",
    "tt_gdp_df['date'] = tt_gdp_df['date'].astype(\"int64\")\n",
    "\n",
    "tt_gdp_df = tt_gdp_df.rename(columns={\"date\": \"Year\"})\n",
    "tt_gdp_df = tt_gdp_df.rename(columns={\"value\": \"GDP_(US$)\"})\n",
    "\n",
    "tt_gdp_df = tt_gdp_df.drop(columns={\"country\", \"countryiso3code\", \"unit\", \"indicator\", \"obs_status\", \"decimal\", \"Unnamed: 0\"})\n",
    "\n",
    "#only keeping years 2000-2025\n",
    "tt_gdp_df = tt_gdp_df[tt_gdp_df['Year'].between(2000, 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91867cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomic_df = pd.merge(socioeconomic_df, tt_gdp_df, on='Year', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "socioeconomic_df[\"GDP (Per Capita TT$)\"] = socioeconomic_df[\"GDP (Per Capita TT$)\"].str.replace(',', '').astype(float)\n",
    "\n",
    "#to USD\n",
    "socioeconomic_df[\"GDP (Per Capita TT$)\"] = (socioeconomic_df[\"GDP (Per Capita TT$)\"] / 6.8).round(2)\n",
    "socioeconomic_df = socioeconomic_df.rename(columns={\"GDP (Per Capita TT$)\": \"GDP (Per Capita US$)\"})\n",
    "# Display the updated dataframe\n",
    "socioeconomic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomic_df.to_csv(r\"csvs/socioeconomic_df_gdp_added.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_one_year_growth(df, column_name):\n",
    "    df[f'{column_name}_1y'] = np.log(df[column_name] / df[column_name].shift(1))\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0287875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_one_term_growth(df, column_name, election_year_gap):\n",
    "    df[f'{column_name}_1t'] = np.log(df[column_name] / df[column_name].shift(election_year_gap))\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_elec_df = pd.read_csv('csvs/district_division.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "homicide_term_growth = df_long_homicide_by_div.copy()\n",
    "homicide_term_growth = calculate_one_term_growth(homicide_term_growth, 'Murders', 7)\n",
    "homicide_07 = homicide_term_growth[homicide_term_growth['Year'] == 2007] \n",
    "homicide_07 = homicide_07.drop(columns={\"Rate_per_100k_Population\", \"Annual_change\", \"Total_murders_per_year\",'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "homicide_term_growth = df_long_homicide_by_div.copy()\n",
    "homicide_term_growth = calculate_one_term_growth(homicide_term_growth, 'Murders', 10)\n",
    "homicide_10 = homicide_term_growth[homicide_term_growth['Year'] == 2010]\n",
    "homicide_10 = homicide_10.drop(columns={\"Rate_per_100k_Population\", \"Annual_change\", \"Total_murders_per_year\",'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d958659",
   "metadata": {},
   "outputs": [],
   "source": [
    "homicide_term_growth = df_long_homicide_by_div.copy()\n",
    "homicide_term_growth = calculate_one_term_growth(homicide_term_growth, 'Murders', 15)\n",
    "homicide_15 = homicide_term_growth[homicide_term_growth['Year'] == 2015]\n",
    "homicide_15 = homicide_15.drop(columns={\"Rate_per_100k_Population\", \"Annual_change\", \"Total_murders_per_year\",'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "homicide_term_growth = df_long_homicide_by_div.copy()\n",
    "homicide_term_growth = calculate_one_term_growth(homicide_term_growth, 'Murders', 20)\n",
    "homicide_20 = homicide_term_growth[homicide_term_growth['Year'] == 2020]\n",
    "homicide_20 = homicide_20.drop(columns={\"Rate_per_100k_Population\", \"Annual_change\", \"Total_murders_per_year\",'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "homicide_term_growth = df_long_homicide_by_div.copy()\n",
    "homicide_term_growth = calculate_one_term_growth(homicide_term_growth, 'Murders', 24)\n",
    "homicide_25 = homicide_term_growth[homicide_term_growth['Year'] == 2024]\n",
    "homicide_25 = homicide_25.drop(columns={\"Rate_per_100k_Population\", \"Annual_change\", \"Total_murders_per_year\",'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_07['Year'] = 2007\n",
    "soc_info_07 = socioeconomic_df\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'GNI (Per Capita US $)', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'GNI (Per Capita US $)')\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'Inflation', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'Inflation')\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'Unemployment', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'Unemployment')\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'Net Migration Rate', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'Net Migration Rate')\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'GDP (Per Capita US$)', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'GDP (Per Capita US$)')\n",
    "soc_info_07 = calculate_one_term_growth(soc_info_07, 'GDP_(US$)', 7)\n",
    "soc_info_07 = calculate_one_year_growth(soc_info_07, 'GDP_(US$)')\n",
    "soc_07 = soc_info_07[soc_info_07['Year'] == 2007]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_10['Year'] = 2010\n",
    "soc_info_10 = socioeconomic_df\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'GNI (Per Capita US $)', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'GNI (Per Capita US $)')\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'Inflation', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'Inflation')\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'Unemployment', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'Unemployment')\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'Net Migration Rate', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'Net Migration Rate')\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'GDP (Per Capita US$)', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'GDP (Per Capita US$)')\n",
    "soc_info_10 = calculate_one_term_growth(soc_info_10, 'GDP_(US$)', 10)\n",
    "soc_info_10 = calculate_one_year_growth(soc_info_10, 'GDP_(US$)')\n",
    "soc_10 = soc_info_10[soc_info_10['Year'] == 2010]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_15['Year'] = 2015\n",
    "soc_info_15 = socioeconomic_df\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'GNI (Per Capita US $)', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'GNI (Per Capita US $)')\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'Inflation', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'Inflation')\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'Unemployment', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'Unemployment')\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'Net Migration Rate', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'Net Migration Rate')\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'GDP (Per Capita US$)', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'GDP (Per Capita US$)')\n",
    "soc_info_15 = calculate_one_term_growth(soc_info_15, 'GDP_(US$)', 15)\n",
    "soc_info_15 = calculate_one_year_growth(soc_info_15, 'GDP_(US$)')\n",
    "soc_15 = soc_info_15[soc_info_15['Year'] == 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_20['Year'] = 2020\n",
    "soc_info_20 = socioeconomic_df\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'GNI (Per Capita US $)', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'GNI (Per Capita US $)')\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'Inflation', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'Inflation')\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'Unemployment', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'Unemployment')\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'Net Migration Rate', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'Net Migration Rate')\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'GDP (Per Capita US$)', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'GDP (Per Capita US$)')\n",
    "soc_info_20 = calculate_one_term_growth(soc_info_20, 'GDP_(US$)', 20)\n",
    "soc_info_20 = calculate_one_year_growth(soc_info_20, 'GDP_(US$)')\n",
    "soc_20 = soc_info_20[soc_info_20['Year'] == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_25['Year'] = 2024\n",
    "sht_25\n",
    "soc_info_25 = socioeconomic_df\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'GNI (Per Capita US $)', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'GNI (Per Capita US $)')\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'Inflation', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'Inflation')\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'Unemployment', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'Unemployment')\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'Net Migration Rate', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'Net Migration Rate')\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'GDP (Per Capita US$)', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'GDP (Per Capita US$)')\n",
    "soc_info_25 = calculate_one_term_growth(soc_info_25, 'GDP_(US$)', 24)\n",
    "soc_info_25 = calculate_one_year_growth(soc_info_25, 'GDP_(US$)')\n",
    "soc_25 = soc_info_25[soc_info_25['Year'] == 2024]\n",
    "# soc_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70318812",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht_07 = sht_07[['ELECTORAL_DISTRICT','ELECTORATE', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'VOTER_TURNOUT', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "                'VALID_VOTES', 'Year']]\n",
    "sht_07 = pd.get_dummies(sht_07, columns=['ELECTORAL_DISTRICT'], prefix='ELECTORAL_DISTRICT')\n",
    "merged_07 = pd.merge(sht_07, soc_07, on='Year', how='inner')\n",
    "# merged_07 = pd.merge(div_elec_df, merged_07, on='ELECTORAL_DISTRICT', how='inner')\n",
    "# merged_07 = pd.merge(homicide_07, merged_07, on='Division', how='outer')\n",
    "\n",
    "sht_10 = sht_10[['ELECTORAL_DISTRICT','ELECTORATE', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'VOTER_TURNOUT', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "                'VALID_VOTES', 'Year']]\n",
    "sht_10 = pd.get_dummies(sht_10, columns=['ELECTORAL_DISTRICT'], prefix='ELECTORAL_DISTRICT')\n",
    "merged_10 = pd.merge(sht_10, soc_10, on='Year', how='inner')\n",
    "# merged_10 = pd.merge(div_elec_df, merged_10, on='ELECTORAL_DISTRICT', how='inner')\n",
    "# merged_10 = pd.merge(homicide_10, merged_10, on='Division', how='outer')\n",
    "\n",
    "sht_15 = sht_15[['ELECTORAL_DISTRICT','ELECTORATE', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'VOTER_TURNOUT', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "                'VALID_VOTES', 'Year']]\n",
    "sht_15 = pd.get_dummies(sht_15, columns=['ELECTORAL_DISTRICT'], prefix='ELECTORAL_DISTRICT')\n",
    "merged_15 = pd.merge(sht_15, soc_15, on='Year', how='inner')\n",
    "# merged_15 = pd.merge(div_elec_df, merged_15, on='ELECTORAL_DISTRICT', how='inner')\n",
    "# merged_15 = pd.merge(homicide_15, merged_15, on='Division', how='outer')\n",
    "\n",
    "sht_20 = sht_20[['ELECTORAL_DISTRICT','ELECTORATE', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'VOTER_TURNOUT', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "                'VALID_VOTES', 'Year']]\n",
    "sht_20 = pd.get_dummies(sht_20, columns=['ELECTORAL_DISTRICT'], prefix='ELECTORAL_DISTRICT')\n",
    "merged_20 = pd.merge(sht_20, soc_20, on='Year', how='inner')\n",
    "# merged_20 = pd.merge(div_elec_df, merged_20, on='ELECTORAL_DISTRICT', how='inner')\n",
    "# merged_20 = pd.merge(homicide_20, merged_20, on='Division', how='outer')\n",
    "\n",
    "sht_25 = sht_25[['ELECTORAL_DISTRICT','ELECTORATE', 'P.N.M._VOTES_%', 'U.N.C._VOTES_%', 'VOTER_TURNOUT', 'TOTAL_NUMBER_OF_VOTES_CAST',\n",
    "                'VALID_VOTES', 'Year']]\n",
    "sht_25 = pd.get_dummies(sht_25, columns=['ELECTORAL_DISTRICT'], prefix='ELECTORAL_DISTRICT')\n",
    "merged_25 = pd.merge(sht_25, soc_25, on='Year', how='inner')\n",
    "# merged_25 = pd.merge(div_elec_df, merged_25, on='ELECTORAL_DISTRICT', how='inner')\n",
    "# merged_25 = pd.merge(homicide_25, merged_25, on='Division', how='outer')\n",
    "\n",
    "master_df = pd.concat([merged_07, merged_10, merged_15, merged_20, merged_25], ignore_index=True)\n",
    "master_df['Growth Rate'] = master_df['Growth Rate'].str.rstrip('%').astype('float')\n",
    "master_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv(r\"csvs/master_df.csv\", index=False)\n",
    "master_df.info() #growth rate is an object, need to convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23642e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soc_info_07.columns.tolist()) #lazy to type out all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dc373",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "\n",
    "- The below Correlation Matrix graphically illustrates the relationship between key economic indicators and voting behiour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data to include only rows where 'Year' is less than or equal to 2020\n",
    "filtered_df = soc_info_07[soc_info_07['Year'] <= 2020]\n",
    "\n",
    "# Select only numeric features\n",
    "numeric_df = filtered_df.select_dtypes(include='number')\n",
    "numeric_df = numeric_df.dropna(axis=1)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Replace NaN or infinite values in the correlation matrix\n",
    "corr_matrix = corr_matrix.fillna(0)  # Replace NaN with 0\n",
    "corr_matrix = corr_matrix.replace([float('inf'), -float('inf')], 0)  # Replace infinite values with 0\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.clustermap(corr_matrix, annot=True, cmap='coolwarm', figsize=(10, 10), linewidths=0.5)\n",
    "plt.suptitle(\"Clustered Correlation Matrix\", y=1.02)\n",
    "plt.show()\n",
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_list = [\n",
    "    'ARIMA', 'AROUCA/MALONEY', 'BARATARIA/SAN JUAN', 'CARONI CENTRAL', 'CARONI EAST',\n",
    "    'CHAGUANAS EAST', 'CHAGUANAS WEST', 'COUVA NORTH', 'COUVA SOUTH', 'CUMUTO/MANZANILLA',\n",
    "    \"D'ABADIE/O'MEARA\", 'DIEGO MARTIN CENTRAL', 'DIEGO MARTIN NORTH/EAST', 'DIEGO MARTIN WEST',\n",
    "    'FYZABAD', 'LA BREA', 'LA HORQUETTA/TALPARO', 'LAVENTILLE EAST/MORVANT', 'LAVENTILLE WEST',\n",
    "    'LOPINOT/BON AIR WEST', 'MAYARO', 'MORUGA/TABLELAND', 'NAPARIMA', 'OROPOUCHE EAST',\n",
    "    'OROPOUCHE WEST', 'POINT FORTIN', 'POINTE-A-PIERRE', \"PORT-OF-SPAIN NORTH/ST. ANN'S WEST\",\n",
    "    'PORT-OF-SPAIN SOUTH', 'PRINCES TOWN', 'SAN FERNANDO EAST', 'SAN FERNANDO WEST', 'SIPARIA',\n",
    "    \"ST. ANN'S EAST\", 'ST. AUGUSTINE', 'ST. JOSEPH', 'TABAQUITE', 'TOBAGO EAST', 'TOBAGO WEST',\n",
    "    'TOCO/SANGRE GRANDE', 'TUNAPUNA', 'TOTAL'\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Repeat the constituency list\n",
    "repeated_constituencies = constituency_list * 5  # because you said repeat 5 times\n",
    "\n",
    "master_df['CONSTITUENCIES'] = repeated_constituencies\n",
    "\n",
    "cols = ['CONSTITUENCIES'] + [col for col in master_df.columns if col != 'CONSTITUENCIES']\n",
    "master_df = master_df[cols]\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29acfd1",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e68e4",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = master_df.copy()\n",
    "df = df[df['Year'] == 2020]\n",
    "constituencies = [\n",
    "    \"ELECTORAL_DISTRICT_ARIMA\", \"ELECTORAL_DISTRICT_AROUCA/MALONEY\", \"ELECTORAL_DISTRICT_BARATARIA/SAN JUAN\", \"ELECTORAL_DISTRICT_CARONI CENTRAL\", \"ELECTORAL_DISTRICT_CARONI EAST\",\n",
    "    \"ELECTORAL_DISTRICT_CHAGUANAS EAST\", \"ELECTORAL_DISTRICT_CHAGUANAS WEST\", \"ELECTORAL_DISTRICT_COUVA NORTH\", \"ELECTORAL_DISTRICT_COUVA SOUTH\", \"ELECTORAL_DISTRICT_CUMUTO/MANZANILLA\",\n",
    "    \"ELECTORAL_DISTRICT_D'ABADIE/O'MEARA\", \"ELECTORAL_DISTRICT_DIEGO MARTIN CENTRAL\", \"ELECTORAL_DISTRICT_DIEGO MARTIN NORTH/EAST\", \"ELECTORAL_DISTRICT_DIEGO MARTIN WEST\",\n",
    "    \"ELECTORAL_DISTRICT_FYZABAD\", \"ELECTORAL_DISTRICT_LA BREA\", \"ELECTORAL_DISTRICT_LA HORQUETTA/TALPARO\", \"ELECTORAL_DISTRICT_LAVENTILLE EAST/MORVANT\", \"ELECTORAL_DISTRICT_LAVENTILLE WEST\",\n",
    "    \"ELECTORAL_DISTRICT_LOPINOT/BON AIR WEST\", \"ELECTORAL_DISTRICT_MAYARO\", \"ELECTORAL_DISTRICT_MORUGA/TABLELAND\", \"ELECTORAL_DISTRICT_NAPARIMA\", \"ELECTORAL_DISTRICT_OROPOUCHE EAST\",\n",
    "    \"ELECTORAL_DISTRICT_OROPOUCHE WEST\", \"ELECTORAL_DISTRICT_POINT FORTIN\", \"ELECTORAL_DISTRICT_POINTE-A-PIERRE\", \"ELECTORAL_DISTRICT_PORT-OF-SPAIN NORTH/ST. ANN'S WEST\",\n",
    "    \"PORT-OF-SPAIN SOUTH\", \"PRINCES TOWN\", \"SAN FERNANDO EAST\", \"SAN FERNANDO WEST\", \"SIPARIA\",\"ELECTORAL_DISTRICT_ST. ANN'S EAST\", \"ELECTORAL_DISTRICT_ST. AUGUSTINE\", \n",
    "    \"ELECTORAL_DISTRICT_ST. JOSEPH\", \"ELECTORAL_DISTRICT_TABAQUITE\", \"ELECTORAL_DISTRICT_TOBAGO EAST\", \"ELECTORAL_DISTRICT_TOBAGO WEST\", \"ELECTORAL_DISTRICT_TOCO/SANGRE GRANDE\", \n",
    "    \"ELECTORAL_DISTRICT_TUNAPUNA\", \"ELECTORAL_DISTRICT_TOTAL\"\n",
    "]\n",
    "\n",
    "# Step 2: Get indexes where Year == 2024\n",
    "df_2025 = master_df[master_df['Year'] != 2024]\n",
    "indexes_2025 = df_2025.index.tolist()\n",
    "\n",
    "# Step 3: Make key-value pairs\n",
    "index_constituency_mapping = dict(zip(indexes_2025, constituencies))\n",
    "\n",
    "# Add a column for the winning party\n",
    "df['Winning_Party'] = df[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].idxmax(axis=1)\n",
    "\n",
    "# Add a column for the margin of victory\n",
    "df['Margin_of_Victory'] = df[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].apply(\n",
    "    lambda row: row.max() - sorted(row)[-2], axis=1\n",
    ")\n",
    "\n",
    "# Define thresholds\n",
    "safe_margin_threshold = 0.20    # Safe seat if margin of victory > 20%\n",
    "\n",
    "# Identify safe seats\n",
    "safe_seats = df[(df['Margin_of_Victory'] > safe_margin_threshold)]\n",
    "\n",
    "safe_seat_columns = safe_seats.loc[:, safe_seats.columns.str.startswith('ELECTORAL_DISTRICT_')]\n",
    "unique_safe_seats = safe_seat_columns.columns[(safe_seat_columns == True).any()].tolist()\n",
    "\n",
    "# Identify marginal seats\n",
    "marginal_seats = df[df['Margin_of_Victory'] < safe_margin_threshold]\n",
    "marginal_seat_columns = marginal_seats.loc[:, marginal_seats.columns.str.startswith('ELECTORAL_DISTRICT_')]\n",
    "\n",
    "unique_marginal_seats = marginal_seat_columns.columns[(marginal_seat_columns == True).any()].tolist()\n",
    "seat_columns = marginal_seat_columns.columns[(marginal_seat_columns == True).any()]\n",
    "\n",
    "# Create a dictionary where key = index, value = seat name\n",
    "seat_index_mapping = {}\n",
    "\n",
    "for seat in seat_columns:\n",
    "    # Find the index (row) where the seat is True\n",
    "    matching_indexes = marginal_seat_columns.index[marginal_seat_columns[seat] == True].tolist()\n",
    "    for idx in matching_indexes:\n",
    "        seat_index_mapping[idx] = seat\n",
    "\n",
    "unique_safe_seats = [seat for seat in unique_safe_seats if seat not in unique_marginal_seats]\n",
    "unique_marginal_seats.remove('ELECTORAL_DISTRICT_TOTAL')\n",
    "\n",
    "print(\"Number of Safe Seats:\", len(unique_safe_seats))\n",
    "print(\"Number of Marginal Seats:\", len(unique_marginal_seats))\n",
    "\n",
    "seat_mapping_marginal = {}\n",
    "seat_mapping_safe = {}\n",
    "\n",
    "for index, constituency in index_constituency_mapping.items():\n",
    "    if constituency in unique_safe_seats:\n",
    "        seat_mapping_safe[index] = constituency\n",
    "    elif constituency in unique_marginal_seats:\n",
    "        seat_mapping_marginal[index] = constituency\n",
    "\n",
    "seat_mapping_safe, seat_mapping_marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a94742",
   "metadata": {},
   "outputs": [],
   "source": [
    "seat_mapping_safe_df = pd.DataFrame(list(seat_mapping_safe.items()), columns=['Index', 'Constituency_Safe'])\n",
    "seat_mapping_marginal_df = pd.DataFrame(list(seat_mapping_marginal.items()), columns=['Index', 'Constituency_Marginal'])\n",
    "\n",
    "seat_mapping_safe_df['Constituency_Safe'] = seat_mapping_safe_df['Constituency_Safe'].str.replace('ELECTORAL_DISTRICT_', '', regex=False)\n",
    "seat_mapping_marginal_df['Constituency_Marginal'] = seat_mapping_marginal_df['Constituency_Marginal'].str.replace('ELECTORAL_DISTRICT_', '', regex=False)\n",
    "\n",
    "# Join with master_df for safe seats\n",
    "df_safe_seats = master_df.merge(seat_mapping_safe_df, left_on='CONSTITUENCIES', right_on='Constituency_Safe', how='left')\n",
    "df_marginal_seats = master_df.merge(seat_mapping_marginal_df, left_on='CONSTITUENCIES', right_on='Constituency_Marginal', how='left')\n",
    "\n",
    "df_safe_seats = df_safe_seats[df_safe_seats['Constituency_Safe'].notna()]\n",
    "df_marginal_seats = df_marginal_seats[df_marginal_seats['Constituency_Marginal'].notna()]\n",
    "\n",
    "df_safe_seats['Winning_Party'] = df_safe_seats[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].idxmax(axis=1)\n",
    "df_marginal_seats['Winning_Party'] = df_marginal_seats[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].idxmax(axis=1)\n",
    "# Show the resulting DataFrames for safe and marginal seats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leans_pnm = [\n",
    "    \"ARIMA\", \"AROUCA/MALONEY\", \"D'ABADIE/O'MEARA\", \"DIEGO MARTIN CENTRAL\",\n",
    "    \"DIEGO MARTIN NORTH/EAST\", \"DIEGO MARTIN WEST\", \"LA BREA\",\n",
    "    \"LAVENTILLE EAST/MORVANT\", \"LAVENTILLE WEST\", \"LOPINOT/BON AIR WEST\",\n",
    "    \"POINT FORTIN\", \"PORT-OF-SPAIN NORTH/ST. ANN'S WEST\", \"PORT-OF-SPAIN SOUTH\",\n",
    "    \"SAN FERNANDO EAST\", \"ST. ANN'S EAST\", \"TOBAGO WEST\"\n",
    "]\n",
    "\n",
    "leans_unc = [\n",
    "    \"CARONI CENTRAL\", \"CARONI EAST\", \"CHAGUANAS WEST\", \"COUVA NORTH\",\n",
    "    \"COUVA SOUTH\", \"FYZABAD\", \"NAPARIMA\", \"OROPOUCHE EAST\",\n",
    "    \"OROPOUCHE WEST\", \"PRINCES TOWN\", \"SIPARIA\", \"ST. AUGUSTINE\", \"TABAQUITE\"\n",
    "]\n",
    "\n",
    "battleground = [\n",
    "    \"BARATARIA/SAN JUAN\", \"CHAGUANAS EAST\", \"CUMUTO/MANZANILLA\",\n",
    "    \"LA HORQUETTA/TALPARO\", \"MAYARO\", \"MORUGA/TABLELAND\", \"POINTE-A-PIERRE\",\n",
    "    \"SAN FERNANDO WEST\", \"ST. JOSEPH\", \"TOBAGO EAST\", \"TOCO/SANGRE GRANDE\",\n",
    "    \"TUNAPUNA\"\n",
    "]\n",
    "\n",
    "# Make the mapping dictionary\n",
    "safe_constituency_leaning = {}\n",
    "marginal_constituency_leaning = {}\n",
    "unknown_constituency_leaning = {}\n",
    "\n",
    "for seat in master_df['CONSTITUENCIES']:\n",
    "    if seat in leans_pnm:\n",
    "        safe_constituency_leaning[seat] = 2  # Leans PNM\n",
    "    elif seat in leans_unc:\n",
    "        safe_constituency_leaning[seat] = 1  # Leans UNC\n",
    "    elif seat in battleground:\n",
    "        marginal_constituency_leaning[seat] = 3  # Battleground seat\n",
    "    else:\n",
    "        unknown_constituency_leaning[seat] = None  # Unknown\n",
    "\n",
    "df_safe_seats['CONSTITUENCY_LEANING'] = df_safe_seats['CONSTITUENCIES'].map(safe_constituency_leaning)\n",
    "df_marginal_seats['CONSTITUENCY_LEANING'] = df_marginal_seats['CONSTITUENCIES'].map(marginal_constituency_leaning)\n",
    "master_df['CONSTITUENCY_LEANING'] = master_df['CONSTITUENCIES'].map(safe_constituency_leaning).fillna(\n",
    "    master_df['CONSTITUENCIES'].map(marginal_constituency_leaning).fillna(\n",
    "        master_df['CONSTITUENCIES'].map(unknown_constituency_leaning)\n",
    "    )\n",
    ")\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "features = [\n",
    "            'CONSTITUENCY_LEANING','Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'Inflation_1t', 'Inflation_1y', 'Unemployment_1t',\n",
    "            'Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y','ELECTORATE'\n",
    "            ] \n",
    "\n",
    "df_safe_seats_model = df_safe_seats[df_safe_seats['Year'] != 2024]\n",
    "print(df_safe_seats_model['Winning_Party'].unique())\n",
    "print(len(df_safe_seats_model))\n",
    "\n",
    "# Prepare data for classification\n",
    "X_safe = df_safe_seats_model[features]\n",
    "y_safe = df_safe_seats_model['Winning_Party']\n",
    "\n",
    "df_safe_seats_model\n",
    "\n",
    "# Make it binary: 1 if PNM, 0 otherwise\n",
    "y_safe_binary = (y_safe == 'P.N.M._VOTES_%').astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_safe_train, X_safe_test, y_safe_train, y_safe_test = train_test_split(\n",
    "    X_safe, y_safe_binary, \n",
    "    test_size=0.4, \n",
    "    random_state=42, \n",
    "    stratify=y_safe_binary\n",
    ")\n",
    "\n",
    "# Fit a classification model\n",
    "clf = LogisticRegression(solver='saga', max_iter=1000)\n",
    "clf.fit(X_safe_train, y_safe_train)\n",
    "\n",
    "# Make predictions\n",
    "y_safe_pred = clf.predict(X_safe_test)\n",
    "\n",
    "# Evaluate the classification model\n",
    "print(\"Classification Report for Safe Seats:\\n\")\n",
    "print(classification_report(y_safe_test, y_safe_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix for Safe Seats:\\n\")\n",
    "print(confusion_matrix(y_safe_test, y_safe_pred))\n",
    "print(y_safe_binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401556fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CONSTITUENCY_LEANING','Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'Inflation_1t', 'Inflation_1y', 'Unemployment_1t',\n",
    "            'Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y',\n",
    "            'ELECTORATE'\n",
    "            ] \n",
    "\n",
    "df_safe_seats_model = df_safe_seats[df_safe_seats['Year'] != 2024]\n",
    "print(len(df_safe_seats_model))\n",
    "\n",
    "# Prepare data for classification\n",
    "X_safe = df_safe_seats_model[features]\n",
    "y_safe = df_safe_seats_model['Winning_Party']\n",
    "\n",
    "# Make it binary: 1 if PNM, 0 otherwise\n",
    "y_safe_binary = (y_safe == 'P.N.M._VOTES_%').astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_safe_train, X_safe_test, y_safe_train, y_safe_test = train_test_split(\n",
    "    X_safe, y_safe_binary, \n",
    "    test_size=0.4, \n",
    "    random_state=42, \n",
    "    stratify=y_safe_binary\n",
    ")\n",
    "\n",
    "# Fit a classification model\n",
    "clf = LogisticRegression(solver='saga', max_iter=1000)\n",
    "clf.fit(X_safe_train, y_safe_train)\n",
    "\n",
    "# Make predictions\n",
    "y_safe_pred = clf.predict(X_safe_test)\n",
    "\n",
    "# Evaluate the classification model\n",
    "print(\"Classification Report for Safe Seats:\\n\")\n",
    "print(classification_report(y_safe_test, y_safe_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix for Safe Seats:\\n\")\n",
    "print(confusion_matrix(y_safe_test, y_safe_pred))\n",
    "print(y_safe_binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85893dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers for Logistic Regression\n",
    "    'max_iter': [100, 500, 1000],  # Number of iterations\n",
    "    'class_weight': ['balanced', None]  # Handling class imbalance\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_safe_train, y_safe_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(\"Best hyperparameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_safe_pred = best_logreg.predict(X_safe_test)\n",
    "\n",
    "# Evaluate the model with a classification report and confusion matrix\n",
    "print(\"Classification Report for Safe Seats with Tuning:\\n\")\n",
    "print(classification_report(y_safe_test, y_safe_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix for Safe Seats with Tuning:\\n\")\n",
    "print(confusion_matrix(y_safe_test, y_safe_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the same feature columns as used in training\n",
    "features = ['CONSTITUENCY_LEANING', 'Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate',\n",
    "            'Inflation_1t', 'Inflation_1y', 'Unemployment_1t', 'Unemployment_1y',\n",
    "            'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t',\n",
    "            'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y', 'ELECTORATE']\n",
    "\n",
    "# Filter safe_seats to only include rows for 2025\n",
    "safe_seats_2025_filtered = master_df[master_df['Year'] == 2024]\n",
    "safe_seats_2025_filtered = safe_seats_2025_filtered[(safe_seats_2025_filtered['CONSTITUENCY_LEANING'] == 1) | (safe_seats_2025_filtered['CONSTITUENCY_LEANING'] == 2)]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(f\"Rows after filtering for non-NaN 'Constituency_Leaning': {safe_seats_2025_filtered.shape[0]}\")\n",
    "\n",
    "# Select the same features for prediction\n",
    "X_2025 = safe_seats_2025_filtered[features].copy()\n",
    "\n",
    "\n",
    "# Scale using a scaler **fitted on X_safe**\n",
    "scaler = StandardScaler()\n",
    "X_safe_scaled = scaler.fit_transform(X_safe)        # fit on training data\n",
    "X_2025_scaled = scaler.transform(X_2025)            # transform test (2025) data\n",
    "\n",
    "# Use the best model from GridSearchCV or RandomizedSearchCV\n",
    "best_model = grid_search.best_estimator_  # Use the model from GridSearchCV\n",
    "\n",
    "# Predict class (0 or 1) using the tuned model\n",
    "safe_seat_predictions = best_model.predict(X_2025_scaled)\n",
    "\n",
    "# Optional: decode to party names\n",
    "party_mapping = {1: 'P.N.M.', 0: 'U.N.C.'}\n",
    "decoded_safe_predictions = [party_mapping[pred] for pred in safe_seat_predictions]\n",
    "\n",
    "# Put predictions in a DataFrame\n",
    "predictions_2025 = pd.DataFrame({\n",
    "    'Constituency': safe_seats_2025_filtered['CONSTITUENCIES'],  # assuming index is constituency name\n",
    "    'Predicted_Winning_Party': decoded_safe_predictions\n",
    "})\n",
    "\n",
    "# View the predictions\n",
    "print(y_safe_train.value_counts())\n",
    "print(\"Model Coefficients:\\n\", clf.coef_)\n",
    "y_safe_train_pred = clf.predict(X_safe_train)\n",
    "print(\"Training Accuracy: \", accuracy_score(y_safe_train, y_safe_train_pred))\n",
    "\n",
    "# predictions_2025\n",
    "predictions_2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ce90a",
   "metadata": {},
   "source": [
    "### Multinomial Linear Regression\n",
    "\n",
    "##### PNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f10552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Features\n",
    "df_marginal_seats_model = df_marginal_seats[df_marginal_seats['Year'] != 2024]\n",
    "X_marginal = df_marginal_seats_model[features]\n",
    "\n",
    "# 2. Target (both UNC and PNM)\n",
    "y_marginal = df_marginal_seats_model[['U.N.C._VOTES_%', 'P.N.M._VOTES_%']]\n",
    "\n",
    "# 3. Train-test split\n",
    "X_marginal_train, X_marginal_test, y_marginal_train, y_marginal_test = train_test_split(X_marginal, y_marginal, test_size=0.35, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# 5. Predict for test set\n",
    "y_marginal_pred_all = reg.predict(X_marginal)\n",
    "\n",
    "# Flatten the arrays for actual and predicted values (for the entire dataset)\n",
    "actual_unc_all = y_marginal['U.N.C._VOTES_%'].values\n",
    "predicted_unc_all = y_marginal_pred_all[:, 0]  # UNC predictions (first column)\n",
    "\n",
    "actual_pnm_all = y_marginal['P.N.M._VOTES_%'].values\n",
    "predicted_pnm_all = y_marginal_pred_all[:, 1]  # PNM predictions (second column)\n",
    "\n",
    "# 6. Evaluate the performance (MSE and R^2)\n",
    "mse_unc_all = mean_squared_error(actual_unc_all, predicted_unc_all)\n",
    "r2_unc_all = r2_score(actual_unc_all, predicted_unc_all)\n",
    "\n",
    "mse_pnm_all = mean_squared_error(actual_pnm_all, predicted_pnm_all)\n",
    "r2_pnm_all = r2_score(actual_pnm_all, predicted_pnm_all)\n",
    "\n",
    "print(f\"Mean Squared Error (UNC) on entire dataset: {mse_unc_all}\")\n",
    "print(f\"R-squared (UNC) on entire dataset: {r2_unc_all}\")\n",
    "\n",
    "print(f\"Mean Squared Error (PNM) on entire dataset: {mse_pnm_all}\")\n",
    "print(f\"R-squared (PNM) on entire dataset: {r2_pnm_all}\")\n",
    "\n",
    "# 6. Create a DataFrame with the results\n",
    "marginal_results_df_all = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats_model['CONSTITUENCIES'],\n",
    "    'Actual_UNC_Vote_Share': actual_unc_all,\n",
    "    'Predicted_UNC_Vote_Share': predicted_unc_all,\n",
    "    'Actual_PNM_Vote_Share': actual_pnm_all,\n",
    "    'Predicted_PNM_Vote_Share': predicted_pnm_all\n",
    "})\n",
    "\n",
    "print(\"\\nMarginal Seats Regression Results:\\n\")\n",
    "marginal_results_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dfcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with feature scaling (StandardScaler) and Ridge regression\n",
    "model = Ridge()\n",
    "\n",
    "# Define the expanded parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100],  # Regularization strength for Ridge\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model using the training data\n",
    "grid_search.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_marginal_pred_tuned = best_model.predict(X_marginal)\n",
    "\n",
    "# Flatten the arrays for actual and predicted values (separating UNC and PNM predictions)\n",
    "actual_unc = y_marginal['U.N.C._VOTES_%'].values\n",
    "predicted_unc = y_marginal_pred_tuned[:, 0]  # UNC predictions (first column)\n",
    "\n",
    "actual_pnm = y_marginal['P.N.M._VOTES_%'].values\n",
    "predicted_pnm = y_marginal_pred_tuned[:, 1]  # PNM predictions (second column)\n",
    "\n",
    "# Compute the MSE and R² for both UNC and PNM\n",
    "mse_unc = mean_squared_error(actual_unc, predicted_unc)\n",
    "r2_unc = r2_score(actual_unc, predicted_unc)\n",
    "\n",
    "mse_pnm = mean_squared_error(actual_pnm, predicted_pnm)\n",
    "r2_pnm = r2_score(actual_pnm, predicted_pnm)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) for UNC predictions: {mse_unc}\")\n",
    "print(f\"R-squared (R²) for UNC predictions: {r2_unc}\")\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) for PNM predictions: {mse_pnm}\")\n",
    "print(f\"R-squared (R²) for PNM predictions: {r2_pnm}\")\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values for both parties\n",
    "marginal_results_df_tuned = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats_model['CONSTITUENCIES'],\n",
    "    'Actual_UNC_Vote_Share': actual_unc,\n",
    "    'Predicted_UNC_Vote_Share': predicted_unc,\n",
    "    'Actual_PNM_Vote_Share': actual_pnm,\n",
    "    'Predicted_PNM_Vote_Share': predicted_pnm\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTuned Model Marginal Seats Regression Results:\\n\")\n",
    "marginal_results_df_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Mean Squared Error for both UNC and PNM\n",
    "mse_unc = mean_squared_error(actual_unc, predicted_unc)\n",
    "mse_pnm = mean_squared_error(actual_pnm, predicted_pnm)\n",
    "\n",
    "# 2. R-squared (R²) for both UNC and PNM\n",
    "r2_unc = r2_score(actual_unc, predicted_unc)\n",
    "r2_pnm = r2_score(actual_pnm, predicted_pnm)\n",
    "\n",
    "# 3. Residuals for analysis\n",
    "residuals_unc = actual_unc - predicted_unc\n",
    "residuals_pnm = actual_pnm - predicted_pnm\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 4. Residual plot for UNC\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=predicted_unc, y=residuals_unc, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals for UNC Vote Share')\n",
    "plt.xlabel('Predicted UNC Vote Share')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# 5. Residual plot for PNM\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=predicted_pnm, y=residuals_pnm, color='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals for PNM Vote Share')\n",
    "plt.xlabel('Predicted PNM Vote Share')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE) for UNC: {mse_unc}\")\n",
    "print(f\"Mean Squared Error (MSE) for PNM: {mse_pnm}\")\n",
    "print(f\"R-squared (R²) for UNC: {r2_unc}\")\n",
    "print(f\"R-squared (R²) for PNM: {r2_pnm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine the predicted winner based on the predicted vote shares\n",
    "marginal_results_df_tuned['Predicted_Winner'] = np.where(\n",
    "    marginal_results_df_tuned['Predicted_UNC_Vote_Share'] > marginal_results_df_tuned['Predicted_PNM_Vote_Share'], \n",
    "    'UNC',  # UNC wins if its actual vote share is greater\n",
    "    'PNM'   # PNM wins if its actual vote share is greater\n",
    ")\n",
    "\n",
    "# 2. Determine the actual winner based on the actual vote shares\n",
    "marginal_results_df_tuned['Actual_Winner'] = np.where(\n",
    "    marginal_results_df_tuned['Actual_UNC_Vote_Share'] > marginal_results_df_tuned['Actual_PNM_Vote_Share'], \n",
    "    'UNC',  # UNC wins if its actual vote share is greater\n",
    "    'PNM'   # PNM wins if its actual vote share is greater\n",
    ")\n",
    "\n",
    "# 3. Print the predicted and actual winners for each constituency\n",
    "print(marginal_results_df_tuned[['Constituency', 'Actual_Winner', 'Predicted_Winner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine the predicted winner based on the predicted vote shares\n",
    "marginal_results_df_all['Predicted_Winner'] = np.where(\n",
    "    marginal_results_df_all['Predicted_UNC_Vote_Share'] > marginal_results_df_all['Predicted_PNM_Vote_Share'], \n",
    "    'UNC',  # UNC wins if its actual vote share is greater\n",
    "    'PNM'   # PNM wins if its actual vote share is greater\n",
    ")\n",
    "\n",
    "# 2. Determine the actual winner based on the actual vote shares\n",
    "marginal_results_df_all['Actual_Winner'] = np.where(\n",
    "    marginal_results_df_all['Actual_UNC_Vote_Share'] > marginal_results_df_all['Actual_PNM_Vote_Share'], \n",
    "    'UNC',  # UNC wins if its actual vote share is greater\n",
    "    'PNM'   # PNM wins if its actual vote share is greater\n",
    ")\n",
    "\n",
    "# 3. Print the predicted and actual winners for each constituency\n",
    "print(marginal_results_df_all[['Constituency', 'Actual_Winner', 'Predicted_Winner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_model_accuracy = (marginal_results_df_all['Predicted_Winner'] == marginal_results_df_all['Actual_Winner']).mean()\n",
    "print(f\"Normal Model Accuracy: {normal_model_accuracy:.2%}\")\n",
    "\n",
    "tuned_model_accuracy = (marginal_results_df_tuned['Predicted_Winner'] == marginal_results_df_tuned['Actual_Winner']).mean()\n",
    "print(f\"Tuned Model Accuracy: {tuned_model_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ed9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 2025 data\n",
    "X_marginal_2025 = df_marginal_seats[df_marginal_seats['Year'] == 2024][features]\n",
    "# Predict vote shares for 2025\n",
    "y_marginal_2025_pred = best_model.predict(X_marginal_2025)\n",
    "\n",
    "# Split predictions into UNC and PNM predictions\n",
    "predicted_unc_2025 = y_marginal_2025_pred[:, 0]\n",
    "predicted_pnm_2025 = y_marginal_2025_pred[:, 1]\n",
    "\n",
    "# Create DataFrame\n",
    "marginal_2025_results_df = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats[df_marginal_seats['Year'] == 2024]['CONSTITUENCIES'],\n",
    "    'Predicted_UNC_Vote_Share': predicted_unc_2025,\n",
    "    'Predicted_PNM_Vote_Share': predicted_pnm_2025\n",
    "})\n",
    "\n",
    "print(marginal_2025_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "\n",
    "df_marginal_seats_model = df_marginal_seats[df_marginal_seats['Year'] != 2024]\n",
    "df_marginal_seats_model = df_marginal_seats_model[df_marginal_seats_model['CONSTITUENCY_LEANING'] == 3]\n",
    "\n",
    "X_marginal = df_marginal_seats_model[features]\n",
    "y_marginal = df_marginal_seats_model['P.N.M._VOTES_%']\n",
    "\n",
    "# Split the data\n",
    "X_marginal_train, X_marginal_test, y_marginal_train, y_marginal_test = train_test_split(X_marginal, y_marginal, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit a regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# Make predictions\n",
    "y_marginal_pred = reg.predict(X_marginal_test)\n",
    "\n",
    "# Evaluate the regression model\n",
    "print(\"Mean Squared Error for Marginal Seats: \", mean_squared_error(y_marginal_test, y_marginal_pred))\n",
    "print(\"R-squared for Marginal Seats: \", r2_score(y_marginal_test, y_marginal_pred))\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values\n",
    "marginal_results_df = pd.DataFrame({\n",
    "    'Actual_Vote_Share': y_marginal_test,\n",
    "    'Predicted_Vote_Share': y_marginal_pred\n",
    "})\n",
    "\n",
    "print(\"\\nMarginal Seats Regression Results:\\n\")\n",
    "print(marginal_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100],  # Regularization strength\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_ridge = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_marginal_pred = best_ridge.predict(X_marginal_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(\"Mean Squared Error for Marginal Seats (Tuned): \", mean_squared_error(y_marginal_test, y_marginal_pred))\n",
    "print(\"R-squared for Marginal Seats (Tuned): \", r2_score(y_marginal_test, y_marginal_pred))\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values\n",
    "marginal_results_df = pd.DataFrame({\n",
    "    'Actual_Vote_Share': y_marginal_test,\n",
    "    'Predicted_Vote_Share': y_marginal_pred\n",
    "})\n",
    "\n",
    "print(\"\\nMarginal Seats Regression Results (Tuned):\\n\")\n",
    "print(marginal_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for 2025\n",
    "df_marginal_seats_2025 = master_df[(master_df['Year'] == 2024) & (master_df['CONSTITUENCIES'] != 'TOTAL') & (master_df['CONSTITUENCY_LEANING'] == 3)]\n",
    "\n",
    "# Ensure we only use the same features that were used in training\n",
    "X_marginal_2025 = df_marginal_seats_2025[features]\n",
    "\n",
    "# Predict the vote share for the 2025 marginal seats\n",
    "y_marginal_2025_pred = reg.predict(X_marginal_2025)\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values for 2025\n",
    "marginal_2025_results_df_pnm = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats_2025['CONSTITUENCIES'],\n",
    "    'Actual_Vote_Share': df_marginal_seats_2025['P.N.M._VOTES_%'],  # Actual values, if available\n",
    "    'Predicted_Vote_Share': y_marginal_2025_pred\n",
    "})\n",
    "\n",
    "# Show the results\n",
    "print(\"\\n2025 PNM Marginal Seats Prediction Results:\\n\")\n",
    "print(marginal_2025_results_df_pnm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b74a4",
   "metadata": {},
   "source": [
    "##### UNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d50321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "df_marginal_seats_model = df_marginal_seats[df_marginal_seats['Year'] != 2024]\n",
    "X_marginal = df_marginal_seats_model[features]\n",
    "y_marginal = df_marginal_seats_model['U.N.C._VOTES_%']\n",
    "\n",
    "# Split the data\n",
    "X_marginal_train, X_marginal_test, y_marginal_train, y_marginal_test = train_test_split(X_marginal, y_marginal, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit a regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# Make predictions\n",
    "y_marginal_pred = reg.predict(X_marginal_test)\n",
    "\n",
    "# Evaluate the regression model\n",
    "print(\"Mean Squared Error for Marginal Seats: \", mean_squared_error(y_marginal_test, y_marginal_pred))\n",
    "print(\"R-squared for Marginal Seats: \", r2_score(y_marginal_test, y_marginal_pred))\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values\n",
    "marginal_results_df = pd.DataFrame({\n",
    "    'Actual_Vote_Share': y_marginal_test,\n",
    "    'Predicted_Vote_Share': y_marginal_pred\n",
    "})\n",
    "\n",
    "print(\"\\nMarginal Seats Regression Results:\\n\")\n",
    "print(marginal_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the model\n",
    "# ridge = Ridge()\n",
    "\n",
    "# # Define the parameter grid for tuning\n",
    "# param_grid = {\n",
    "#     'alpha': [0.1, 1, 10, 100],  # Regularization strength\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# # Fit GridSearchCV to the data\n",
    "# grid_search.fit(X_marginal_train, y_marginal_train)\n",
    "\n",
    "# # Get the best hyperparameters from the grid search\n",
    "# print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# # Get the best model from the grid search\n",
    "# best_ridge = grid_search.best_estimator_\n",
    "\n",
    "# # Make predictions using the best model\n",
    "# y_marginal_pred = best_ridge.predict(X_marginal_test)\n",
    "\n",
    "# # Evaluate the tuned model\n",
    "# print(\"Mean Squared Error for Marginal Seats (Tuned): \", mean_squared_error(y_marginal_test, y_marginal_pred))\n",
    "# print(\"R-squared for Marginal Seats (Tuned): \", r2_score(y_marginal_test, y_marginal_pred))\n",
    "\n",
    "# # Create a DataFrame to compare actual and predicted values\n",
    "# marginal_results_df = pd.DataFrame({\n",
    "#     'Actual_Vote_Share': y_marginal_test,\n",
    "#     'Predicted_Vote_Share': y_marginal_pred\n",
    "# })\n",
    "\n",
    "# print(\"\\nMarginal Seats Regression Results (Tuned):\\n\")\n",
    "# print(marginal_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87999897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for 2025\n",
    "df_marginal_seats_2025 = master_df[(master_df['Year'] == 2024) & (master_df['CONSTITUENCIES'] != 'TOTAL') & (master_df['CONSTITUENCY_LEANING'] == 3)]\n",
    "\n",
    "# Ensure we only use the same features that were used in training\n",
    "X_marginal_2025 = df_marginal_seats_2025[features]\n",
    "\n",
    "# Predict the vote share for the 2025 marginal seats\n",
    "y_marginal_2025_pred = reg.predict(X_marginal_2025)\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted values for 2025\n",
    "marginal_2025_results_df_unc = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats_2025['CONSTITUENCIES'],\n",
    "    'Actual_Vote_Share': df_marginal_seats_2025['U.N.C._VOTES_%'],  # Actual values, if available\n",
    "    'Predicted_Vote_Share': y_marginal_2025_pred\n",
    "})\n",
    "\n",
    "# Show the results\n",
    "print(\"\\n2025 UNC Marginal Seats Prediction Results:\\n\")\n",
    "print(marginal_2025_results_df_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recalibrate_predictions(predictions, k=5, center=0.5):\n",
    "    \"\"\"\n",
    "    Apply a sigmoid recalibration to prevent extreme vote share predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (array-like): Raw model predictions.\n",
    "        k (float): Steepness of the sigmoid. Higher = steeper.\n",
    "        center (float): Center point around which to squash.\n",
    "        \n",
    "    Returns:\n",
    "        array-like: Adjusted predictions.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-k * (predictions - center)))\n",
    "\n",
    "y_marginal_2025_pred_recalibrated = recalibrate_predictions(y_marginal_2025_pred, k=5, center=0.5)\n",
    "\n",
    "marginal_2025_results_df_unc_recalibrated = pd.DataFrame({\n",
    "    'Constituency': df_marginal_seats_2025['CONSTITUENCIES'],\n",
    "    'Actual_Vote_Share': df_marginal_seats_2025['U.N.C._VOTES_%'],\n",
    "    'Predicted_Vote_Share_Recalibrated': y_marginal_2025_pred_recalibrated\n",
    "})\n",
    "\n",
    "marginal_2025_results_df_unc_recalibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Predicted PNM > Predicted UNC, PNM wins\n",
    "marginal_2025_results_df = pd.merge(marginal_2025_results_df_unc_recalibrated, marginal_2025_results_df_pnm, on='Constituency')\n",
    "marginal_2025_results_df\n",
    "marginal_2025_results_df['Predicted_Winner'] = np.where(\n",
    "    marginal_2025_results_df['Predicted_Vote_Share'] > marginal_2025_results_df['Predicted_Vote_Share_Recalibrated'], \n",
    "    'PNM', \n",
    "    'UNC'\n",
    ")\n",
    "\n",
    "print(marginal_2025_results_df[['Constituency', 'Predicted_Winner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3683e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import joblib\n",
    "\n",
    "# # === 1. Define features and target ===\n",
    "# features = [\n",
    "#     'Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate',\n",
    "#     'Inflation_1t', 'Inflation_1y', 'Unemployment_1t', 'Unemployment_1y',\n",
    "#     'Net Migration Rate_1t', 'Net Migration Rate_1y',\n",
    "#     'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y'\n",
    "# ]\n",
    "\n",
    "# df_safe_seats_model = df_safe_seats[df_safe_seats['Year'] != 2024]\n",
    "\n",
    "# X_safe = df_safe_seats_model[features]\n",
    "# y_safe = df_safe_seats_model['Winning_Party']\n",
    "\n",
    "# # Encode target\n",
    "# y_safe_encoded = y_safe.astype('category').cat.codes\n",
    "\n",
    "# # === 2. Split data ===\n",
    "# X_safe_train, X_safe_test, y_safe_train, y_safe_test = train_test_split(\n",
    "#     X_safe, y_safe_encoded, test_size=0.4, random_state=42, stratify=y_safe_encoded\n",
    "# )\n",
    "\n",
    "# # (Optional) Scale features — Random Forests don't strictly need scaling, but if you want consistency:\n",
    "# # scaler = StandardScaler()\n",
    "# # X_safe_train = scaler.fit_transform(X_safe_train)\n",
    "# # X_safe_test = scaler.transform(X_safe_test)\n",
    "\n",
    "# # === 3. Define Random Forest Model and GridSearch ===\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [50, 100, 200, 300, 500],\n",
    "#     'max_depth': [None, 5, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10, 20],\n",
    "#     'min_samples_leaf': [1, 2, 5, 10],\n",
    "#     'max_features': ['sqrt', 'log2', 0.8, 0.5],\n",
    "#     'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "# }\n",
    "\n",
    "# grid_search_rf = GridSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_grid=param_grid_rf,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # === 4. Train model ===\n",
    "# grid_search_rf.fit(X_safe_train, y_safe_train)\n",
    "\n",
    "# print(\"Best hyperparameters found: \", grid_search_rf.best_params_)\n",
    "\n",
    "# # === 5. Evaluate ===\n",
    "# best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# y_safe_pred_rf = best_rf.predict(X_safe_test)\n",
    "\n",
    "# print(\"\\nClassification Report for Safe Seats (Random Forest):\\n\")\n",
    "# print(classification_report(y_safe_test, y_safe_pred_rf))\n",
    "\n",
    "# print(\"\\nConfusion Matrix for Safe Seats (Random Forest):\\n\")\n",
    "# print(confusion_matrix(y_safe_test, y_safe_pred_rf))\n",
    "\n",
    "# # === 6. Save model if needed ===\n",
    "# joblib.dump(best_rf, \"random_forest_safe_seats_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c155fe8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# features = ['Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'Inflation_1t', 'Inflation_1y', \n",
    "#             'Unemployment_1t','Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "#             'GDP_(US$)_1y', 'VOTER_TURNOUT', 'VALID_VOTES'] \n",
    "\n",
    "# # print(features)\n",
    "\n",
    "# X = master_df[features]\n",
    "# print(X.columns.tolist())\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# if 'VOTER_TURNOUT' not in socioeconomic_df.columns:\n",
    "#     socioeconomic_df['VOTER_TURNOUT'] = 0\n",
    "# if 'VALID_VOTES' not in socioeconomic_df.columns:\n",
    "#     socioeconomic_df['VALID_VOTES'] = 0\n",
    "\n",
    "# # Select features for 2025\n",
    "# X_2025 = socioeconomic_df[socioeconomic_df['Year'] == 2025][features]\n",
    "\n",
    "\n",
    "\n",
    "# # Ensure `X_2024` has the same features as the training data\n",
    "# X_2025_aligned = pd.DataFrame(columns=features, index=X_2025.index)   # Create an empty DataFrame with training features\n",
    "# X_2025_aligned.update(X_2025)  # Update with the values from `X_2025`\n",
    "# X_2025_aligned.fillna(0, inplace=True)  # Fill missing features with 0\n",
    "\n",
    "\n",
    "# # Reorder columns to match the training data\n",
    "# X_2025_aligned = X_2025_aligned[features]\n",
    "\n",
    "# # Scale the input data if scaling was applied during training\n",
    "# X_2025_scaled = scaler.transform(X_2025_aligned)\n",
    "\n",
    "# print(X_2025_scaled)\n",
    "\n",
    "# # Predict the winning party for 2024\n",
    "# winning_party_2025 = clf.predict(X_2025_scaled)\n",
    "\n",
    "# # Decode the prediction if the target variable was encoded\n",
    "# party_mapping = {0: 'P.N.M.', 1: 'U.N.C.'}  # Adjust based on your encoding\n",
    "# decoded_predictions = [party_mapping[pred] for pred in winning_party_2025]\n",
    "\n",
    "# # Count the number of seats won by each party\n",
    "# seat_breakdown = pd.Series(decoded_predictions).value_counts()\n",
    "\n",
    "# # Print the breakdown of seats\n",
    "# print(\"Breakdown of Seats Won in 2025:\")\n",
    "# print(seat_breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315aa02",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "##### Marginal Seats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd77e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# df = safe_seats.copy()\n",
    "\n",
    "# target = 'P.N.M._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = df.drop(['P.N.M._VOTES_%', 'U.N.C._VOTES_%','Winning_Party', 'Margin_of_Victory'], axis=1)\n",
    "# y = df[target]\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# svr = SVR(kernel='linear') \n",
    "# svr.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svr.predict(X_test)\n",
    "\n",
    "# #mse for eval\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f\"R^2 score: {r2}\")\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())\n",
    "\n",
    "# print(df[features].corrwith(df['P.N.M._VOTES_%']))\n",
    "# print(df[features].corrwith(df['U.N.C._VOTES_%']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d36bc1",
   "metadata": {},
   "source": [
    "Graphical plot of SVM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.scatter(y_test, y_pred)\n",
    "# plt.xlabel(\"Actual\")\n",
    "# plt.ylabel(\"Predicted\")\n",
    "# plt.title(\"SVM Predictions\")\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # ideal predictions line\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe4b76",
   "metadata": {},
   "source": [
    "### Support Vector Machine \n",
    "\n",
    "##### Safe Seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad977d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = master_df.copy()\n",
    "\n",
    "# target = 'U.N.C._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = df.drop(['U.N.C._VOTES_%', 'P.N.M._VOTES_%'], axis=1)\n",
    "# y = df[target]\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # initialize and train the Support Vector Regression model\n",
    "# svr = SVR(kernel='linear')  # change kernel as needed\n",
    "# svr.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svr.predict(X_test)\n",
    "\n",
    "# #mse for eval\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f\"R² score: {r2}\")\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921d00f",
   "metadata": {},
   "source": [
    "Graphical plot of SVM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f584d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.scatter(y_test, y_pred)\n",
    "# plt.xlabel(\"Actual\")\n",
    "# plt.ylabel(\"Predicted\")\n",
    "# plt.title(\"SVM Predictions\")\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # ideal predictions line\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348dc2a",
   "metadata": {},
   "source": [
    "### Decision Tree Regression and Classifiers w/o marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129e8dc",
   "metadata": {},
   "source": [
    "##### Decision Tree Regressor w/o marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49287a96",
   "metadata": {},
   "source": [
    "##### PNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfa161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn import metrics\n",
    "\n",
    "# decisions = master_df.copy()\n",
    "\n",
    "# feat_cols = ['Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'Inflation_1t', 'Inflation_1y', \n",
    "#             'Unemployment_1t','Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "#             'GDP_(US$)_1y', 'VOTER_TURNOUT', 'VALID_VOTES'] # All socioeconomic\n",
    "# # feat_cols = ['Year', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'GDP_(US$)_1t',\n",
    "# #             'GDP_(US$)_1y'] # National Economic trends\n",
    "# # feat_cols = ['Inflation', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "# #             'GDP_(US$)_1y'] # Household Purchasing Power\n",
    "\n",
    "# target = 'P.N.M._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = decisions[feat_cols]\n",
    "# y = decisions[target]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # # training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=25)\n",
    "\n",
    "# # model test\n",
    "# regressor = DecisionTreeRegressor()\n",
    "\n",
    "# regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = regressor.predict(X_test)\n",
    "\n",
    "# # performance tests \n",
    "\n",
    "# mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(\"R^2 score:\", metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de746590",
   "metadata": {},
   "source": [
    "##### UNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn import metrics\n",
    "\n",
    "# decisions = master_df.copy()\n",
    "\n",
    "# feat_cols = ['Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'Inflation_1t', 'Inflation_1y', \n",
    "#             'Unemployment_1t','Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "#             'GDP_(US$)_1y', 'VOTER_TURNOUT', 'VALID_VOTES'] # All socioeconomic\n",
    "\n",
    "# target = 'U.N.C._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = decisions[feat_cols]\n",
    "# y = decisions[target]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # # training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=25)\n",
    "\n",
    "# # model test\n",
    "# regressor = DecisionTreeRegressor()\n",
    "\n",
    "# regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = regressor.predict(X_test)\n",
    "\n",
    "# # performance tests \n",
    "\n",
    "# mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(\"R^2 score:\", metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f172d4",
   "metadata": {},
   "source": [
    "### Decision Tree Classisfier w/o marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dbfbe",
   "metadata": {},
   "source": [
    "##### PNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# decisions = master_df.copy()\n",
    "\n",
    "# ['Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'Inflation_1t', 'Inflation_1y', \n",
    "#             'Unemployment_1t','Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "#             'GDP_(US$)_1y', 'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES'] # All socioeconomic\n",
    "\n",
    "# # feat_cols = ['Year', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'GDP_(US$)_1t',\n",
    "# #             'GDP_(US$)_1y'] # National Economic trends\n",
    "# # feat_cols = ['Inflation', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y']  # Household Purchasing Power\n",
    "\n",
    "# target = 'P.N.M._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = decisions[feat_cols] \n",
    "# y = decisions[target]  \n",
    "\n",
    "# # discretize the target variable into bins\n",
    "# discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')  # adjust `n_bins` as needed\n",
    "# y_binned = discretizer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_binned, test_size=0.1, random_state=25)\n",
    "\n",
    "# # model test\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # performance metrics \n",
    "# accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "# print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb77a97",
   "metadata": {},
   "source": [
    "##### UNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc419704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# decisions = master_df.copy()\n",
    "\n",
    "# ['Year', 'Homicide_rate_per_100k', 'Annual (Percentage Change)', 'Inflation', 'Net Migration Rate', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'Inflation_1t', 'Inflation_1y', \n",
    "#             'Unemployment_1t','Unemployment_1y', 'Net Migration Rate_1t', 'Net Migration Rate_1y', 'GDP (Per Capita US$)_1t', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t',\n",
    "#             'GDP_(US$)_1y', 'VOTER_TURNOUT', 'REJECTED_BALLOTS', 'VALID_VOTES'] # All socioeconomic\n",
    "\n",
    "# # feat_cols = ['Year', 'GNI (Per Capita US $)_1t', 'GNI (Per Capita US $)_1y', 'GDP_(US$)_1t',\n",
    "# #             'GDP_(US$)_1y'] # National Economic trends\n",
    "# # feat_cols = ['Inflation', 'GDP (Per Capita US$)_1y', 'GDP_(US$)_1t', 'GDP_(US$)_1y']  # Household Purchasing Power\n",
    "\n",
    "# target = 'U.N.C._VOTES_%'  # P.N.M. vote share as the target\n",
    "\n",
    "# X = decisions[feat_cols] \n",
    "# y = decisions[target]  \n",
    "\n",
    "# # discretize the target variable into bins\n",
    "# discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')  # adjust `n_bins` as needed\n",
    "# y_binned = discretizer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_binned, test_size=0.1, random_state=25)\n",
    "\n",
    "# # model test\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # performance metrics \n",
    "# accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "# print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18734430",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor and Classifier based on marginal Seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e249af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = master_df.copy()\n",
    "\n",
    "# df['Winning_Party'] = df[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].idxmax(axis=1)\n",
    "\n",
    "# # adding a column for aa the margin of victory\n",
    "# df['Margin_of_Victory'] =  df[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].apply(\n",
    "#     lambda row: row.max() -  sorted(row)[-2], axis=1\n",
    "# )\n",
    "\n",
    "# # thresholds \n",
    "# safe_vote_share_threshold = 0.60 # Safe seat if vote share > 60%\n",
    "# safe_margin_threshold = 0.20 # Safe seat if margin of victory > 20%\n",
    "# marginal_margin_threshold = 0.10 # Margnal seat if margin of victory < 10%\n",
    "\n",
    "# # identify safe seats\n",
    "# safe_seats = df[\n",
    "#     (df['Margin_of_Victory'] > safe_margin_threshold) |\n",
    "#     (df[['P.N.M._VOTES_%', 'U.N.C._VOTES_%']].max(axis=1) > safe_vote_share_threshold)\n",
    "# ]\n",
    "\n",
    "# # marginal seats \n",
    "# marginal_seats = df[df['Margin_of_Victory'] < marginal_margin_threshold]\n",
    "\n",
    "# print(\"Number of Safe Seats: \", len(safe_seats))\n",
    "# print(\"Number of Marginal Seats: \", len(marginal_seats))\n",
    "\n",
    "# df.to_csv(r\"csvs/master_df_with_seat_analysis.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492033e",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor w/ marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_safe = safe_seats[feat_cols]\n",
    "# y_safe = safe_seats['P.N.M._VOTES_%']\n",
    "# z_safe = safe_seats['U.N.C._VOTES_%']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_safe_scaled = scaler.fit_transform(X_safe)\n",
    "\n",
    "# # training set and test set\n",
    "# X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(X_safe_scaled, y_safe, test_size=0.2, random_state=42)\n",
    "# X_u_train, X_u_test, y_u_train, y_u_test = train_test_split(X_safe_scaled, z_safe, test_size=0.2, random_state=42)\n",
    "\n",
    "# # model train \n",
    "# # P.N.M.\n",
    "# p_regressor = DecisionTreeRegressor(random_state=42)\n",
    "# p_regressor.fit(X_p_train, y_p_train)\n",
    "\n",
    "# # U.N.C.\n",
    "# p_regressor = DecisionTreeRegressor(random_state=42)\n",
    "# p_regressor.fit(X_u_train, y_u_train)\n",
    "\n",
    "# y_p_pred = regressor.predict(X_p_test)\n",
    "# y_u_pred = regressor.predict(X_u_test)\n",
    "\n",
    "# # performance tests\n",
    "# mse_p = mean_squared_error(y_p_test, y_p_pred)\n",
    "# mse_u = mean_squared_error(y_u_test, y_u_pred)\n",
    "\n",
    "# r2_p = r2_score(y_p_test, y_p_pred)\n",
    "# r2_u = r2_score(y_u_test, y_u_pred)\n",
    "\n",
    "# print(f\"Mean Squared Error PNM: {mse_p}  UNC: {mse_u}\")\n",
    "# print(f\"R² Score PNM: {r2_p} UNC: {r2_u}\")\n",
    "\n",
    "# # actual vs predicted values\n",
    "# print(\"PNM\")\n",
    "# results = pd.DataFrame({'Actual': y_p_test, 'Predicted': y_p_pred})\n",
    "# print(results.head())\n",
    "\n",
    "# print(\"UNC\")\n",
    "# results_u = pd.DataFrame({'Actual': y_u_test, 'Predicted': y_u_pred})\n",
    "# print(results_u.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d70a7",
   "metadata": {},
   "source": [
    "### Decision Tree Classisfier w/ marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# # Model Evaluation\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# X_safe = safe_seats[feat_cols]\n",
    "# y_safe = safe_seats['Winning_Party']\n",
    "\n",
    "# y_safe_encoded = y_safe.astype('category').cat.codes\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_safe_scaled = scaler.fit_transform(X_safe)\n",
    "\n",
    "# # Split dataset into training set and test set\n",
    "# X_safe_train, X_safe_test, y_safe_train, y_safe_test = train_test_split(X_safe_scaled, y_safe_encoded, test_size=0.2, random_state=25)\n",
    "\n",
    "# classifier = DecisionTreeClassifier(random_state=25)\n",
    "# classifier = classifier.fit(X_safe_train, y_safe_train)\n",
    "\n",
    "# # Predict response for test dataset\n",
    "# y_safe_pred = classifier.predict(X_safe_test)\n",
    "\n",
    "\n",
    "# accuracy = accuracy_score(y_safe_test, y_safe_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_safe_test, y_safe_pred))\n",
    "\n",
    "# results = pd.DataFrame({'Actual': y_safe_test, 'Predicted': y_safe_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c10261",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # taken from datacamp\n",
    "# # Install the required package\n",
    "# # %pip install pydotplus\n",
    "\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "# import pydotplus\n",
    "\n",
    "# dot_data = StringIO()\n",
    "# export_graphviz(clf, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True,feature_names = feat_cols,class_names=['0','1'])\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# graph.write_png('elections.png')\n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4a0df",
   "metadata": {},
   "source": [
    "### BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a37914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_safe = safe_seats[feat_cols]\n",
    "# y_safe = safe_seats['P.N.M._VOTES_%']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_safe_scaled = scaler.fit_transform(X_safe)\n",
    "\n",
    "# # training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_safe_scaled, y_safe, test_size=0.2, random_state=42)\n",
    "\n",
    "# # model train \n",
    "# regressor = DecisionTreeRegressor(random_state=42)\n",
    "# regressor.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = regressor.predict(X_test)\n",
    "\n",
    "# # performance tests\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"R² Score: {r2}\")\n",
    "\n",
    "# # actual vs predicted values\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e40b21",
   "metadata": {},
   "source": [
    "### MLP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# df = safe_seats.copy()\n",
    "\n",
    "# X = df[features]  \n",
    "# y = df['P.N.M._VOTES_%']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(100,), (100, 30), (50, 50), (150, 75, 30)],\n",
    "#     'activation': ['relu', 'tanh'],\n",
    "#     'solver': ['adam', 'sgd'],\n",
    "#     'alpha': [0.0001, 0.001, 0.01],\n",
    "#     'learning_rate': ['constant', 'adaptive']\n",
    "# }\n",
    "\n",
    "# mlp = MLPRegressor(max_iter=1000, random_state=42)\n",
    "\n",
    "# grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(hidden_layer_sizes=(100,30), \n",
    "#                    activation='relu',  \n",
    "#                    solver='adam',      \n",
    "#                    alpha=0.0001,       \n",
    "#                    max_iter=1000,\n",
    "#                    random_state=42,\n",
    "#                    learning_rate= 'constant')\n",
    "\n",
    "\n",
    "# mlp.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = mlp.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MSE: {mse}\")\n",
    "# print(f\"R² Score: {r2}\")\n",
    "\n",
    "# print(\"Actual vs Predicted:\")\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
